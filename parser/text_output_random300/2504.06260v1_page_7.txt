Preprint. Under review.
2024) are tested on the ModelSpecs task under FEABench Gold, with a one-shot prompt
(Table 2 and 3). We find that closed-weight models are able to generate code with moderate
executability ∼0.60 −0.79, implying that LLMs are familiar with the higher-level grammar
and syntax of COMSOL Multiphysics®API calls or can infer it from the one-shot example.
Getting more granular choices correct is more challenging: LLMs are prone to hallucinating
the interface choice (factuality between [0.54-0.85]). The open-weights LLMs generally
perform worse, especially on the alignment-probing metrics like the Model Tree Score and
Physics Recall1. We also compare the performance of the closed-source LLMs on the 200
problems in FEABench Large. Unlike the human-verified FEABench Gold, FEABench Large
instances do not have a single final target artifact, so we only evaluate these against metrics
Table 2: Code Metrics: Comparison on ModelSpecs across LLMs.
Experiment
Executability
Model Tree Score
Valid Target
Claude 3.5 Sonnet
0.79±0.03
0.69±0.07
1/15
GPT-4o
0.78±0.03
0.56±0.06
0/15
Gemini-1.5-Pro
0.60±0.05
0.46±0.07
0/15
Gemma-2-27B-IT
0.56±0.05
0.47±0.07
0/15
Gemma-2-9B-IT
0.44±0.06
0.28±0.06
0/15
CodeGemma-7B-IT
0.52±0.07
0.35±0.06
0/15
Table 3: Physics Metrics: Comparison on ModelSpecs across LLMs.
Experiment
Interface
Factuality
Interface
Recall
Feature
Recall
Feature
Property
Recall
Feature
Dimension
Claude 3.5 Sonnet
0.85±0.10
0.71±0.13
0.80±0.10
0.22±0.10
0.95±0.05
GPT-4o
0.79±0.11
0.64±0.13
0.55±0.12
0.22±0.11
0.95±0.05
Gemini-1.5-Pro
0.54±0.14
0.43±0.14
0.39±0.10
0.15±0.09
0.86±0.14
Gemma-2-27B-IT
0.69±0.13
0.50±0.14
0.14±0.08
0.11±0.07
-
Gemma-2-9B-IT
0.70±0.15
0.43±0.14
0.06±0.04
0.07±0.07
-
CodeGemma-7B-IT
0.45±0.13
0.21±0.11
0.17±0.09
0.07±0.07
-
Table 4: Comparison across models on FEABench Large.
Experiment
Interface
Factuality
Interface
Recall
Feature
Recall
Feature
Property
Recall
Feature
Dim.
Claude 3.5 Sonnet
0.68±0.03
0.50±0.03
0.49±0.03
0.29±0.02
0.96±0.01
GPT-4o
0.66±0.03
0.48±0.03
0.26±0.03
0.20±0.02
0.82±0.05
Gemini-1.5-Pro
0.57±0.04
0.28±0.03
0.44±0.03
0.20±0.02
0.72±0.04
that don’t require execution. Claude 3.5-Sonnet consistently has the best performance across
metrics on both benchmarks.
5.1
Analyzing factors that contribute to complexity
To analyze sources of difficulty in a single-query, non-interactive setting, first, we use the two
task versions to decouple whether the bottleneck lies in making correct reasoning decisions
or in translating explicit natural language steps into syntactically correct code. Next, we
examine gains from changing the prompting strategy. Finally, we leverage the common
structure of the code across problems to examine which block is the most challenging. We
fix the LLM to Gemini-1.5-Pro in subsequent COMSOL Multiphysics®-based experiments.
We include a detailed qualitative analysis of a single solution in Appendix F.
1Since these LLMs were unable to define matching features (feature recall), the dimension metric
could be evaluated for fewer than 5 problems and was thus omitted in Table 3.
7
