Physics-informed KAN PointNet
particularly noticeable for the ğ‘¢-component of the velocity vector, where the relative error exceeds 10%, surpassing the
commonly accepted engineering threshold. However, the errors in the pressure and temperature fields remain within
acceptable limits. We conclude that reducing the depth of PI-KAN-PointNet significantly impacts its performance, in
contrast to the application of PointNet with KAN in part segmentation tasks in computer graphics.
global 
feature
Shared MLP (ğ‘›ğ‘ Ã— 64, ğ‘›ğ‘ Ã— 64)
Shared MLP (ğ‘›ğ‘ Ã— 64, ğ‘›ğ‘ Ã— 128, ğ‘›ğ‘ Ã— 1024)
shared
shared
Shared KAN (ğ‘›ğ‘ Ã— 512, ğ‘›ğ‘ Ã— 256, ğ‘›ğ‘ Ã— 128)
ğ‘›ğ‘ Ã— 1024
Input
shared
shared
Shared KAN (ğ‘›ğ‘ Ã— 128, ğ‘›PDE)
ğ‘Ã— 2
ğ‘Ã— ğ‘›ğ‘ Ã— 64
ğ‘Ã— ğ‘›ğ‘ Ã— 1024
ğ‘Ã— ğ‘›ğ‘ Ã— 128
ğ‘Ã— ğ‘›PDE
Output
ğ‘Ã— ğ‘›ğ‘ Ã— 1088
Max pool
ğ’™1
ğ’™2
.
.
.
.
ğ’™ğ‘
â‹¯
ğ‘‹1
ğ‘‹2
ğ‘‹ğ‘š
Spatial coordinates
 of ğ‘š geometries
ğ›¿
ğ›¿ğ‘¥
ğ›¿
ğ›¿ğ‘¦
ğ›¿
ğ›¿ğ‘¥
ğ›¿
ğ›¿ğ‘¥
ğ›¿
ğ›¿ğ‘¦
ğ›¿
ğ›¿ğ‘¦
Automatic
differentiation
Physics-informed
 loss function
ğ’™1
ğ’™2
.
.
.
.
ğ’™ğ‘
ğ’™1
ğ’™2
.
.
.
.
ğ’™ğ‘
Figure 11: Architecture of the physics-informed PointNet with shared MLPs in the encoder and shared KANs in the
decoder. In this study, we set ğ‘›ğ‘ = 1 in the encoder, while ğ‘›ğ‘ = 0.5 in the decoder. Other components are defined similarly
to the caption of Fig. 1.
global 
feature
Shared KAN (ğ‘›ğ‘ Ã— 64, ğ‘›ğ‘ Ã— 64)
Shared KAN (ğ‘›ğ‘ Ã— 64, ğ‘›ğ‘ Ã— 128, ğ‘›ğ‘ Ã— 1024)
shared
shared
Shared MLP (ğ‘›ğ‘ Ã— 512, ğ‘›ğ‘ Ã— 256, ğ‘›ğ‘ Ã— 128)
ğ‘›ğ‘ Ã— 1024
Input
shared
shared
Shared MLP (ğ‘›ğ‘ Ã— 128, ğ‘›PDE)
ğ‘Ã— 2
ğ‘Ã— ğ‘›ğ‘ Ã— 64
ğ‘Ã— ğ‘›ğ‘ Ã— 1024
ğ‘Ã— ğ‘›ğ‘ Ã— 128
ğ‘Ã— ğ‘›PDE
Output
ğ‘Ã— ğ‘›ğ‘ Ã— 1088
Max pool
ğ’™1
ğ’™2
.
.
.
.
ğ’™ğ‘
â‹¯
ğ‘‹1
ğ‘‹2
ğ‘‹ğ‘š
Spatial coordinates
 of ğ‘š geometries
ğ›¿
ğ›¿ğ‘¥
ğ›¿
ğ›¿ğ‘¦
ğ›¿
ğ›¿ğ‘¥
ğ›¿
ğ›¿ğ‘¥
ğ›¿
ğ›¿ğ‘¦
ğ›¿
ğ›¿ğ‘¦
Automatic
differentiation
Physics-informed
 loss function
ğ’™1
ğ’™2
.
.
.
.
ğ’™ğ‘
ğ’™1
ğ’™2
.
.
.
.
ğ’™ğ‘
Figure 12: Architecture of the physics-informed PointNet with shared KANs in the encoder and shared MLPs in the
decoder. In this study, we set ğ‘›ğ‘ = 0.5 in the encoder, while ğ‘›ğ‘ = 1.0 in the decoder. Other components are defined similarly
to the caption of Fig. 1.
6.4. Physics-informed PointNet with an MLP (KAN) encoder and KAN (MLP) decoder
After a comprehensive evaluation of PI-KAN-PointNet and an in-depth comparison with physics-informed
PointNet with shared MLPs, an idea worth investigating is the integration of both shared KANs and MLPs into
PointNet to potentially leverage the strengths of both architectures. There are multiple ways to achieve this integration.
One reasonable and straightforward approach is to construct the encoder of PointNet using shared KAN layers
while utilizing shared MLP layers for the decoder, or vice versa. Figure 11 and Figure 12 illustrate the schematic
representation of these two proposed models. Specifically, in shred KAN layers, the Chebyshev polynomial (i.e.,
ğ›¼= ğ›½= âˆ’0.5) with a degree of 2 and ğ‘›ğ‘ = 0.5 was used, while in shared MLP layers, ğ‘›ğ‘ was set to 1.
Table 5 compares these two models in predicting velocity, pressure, and temperature fields across 135 geometries
(ğ‘š= 135). The evaluation, based on relative pointwise error (ğ¿2 norm) over the entire domain and the inner cylinder
surface (for temperature), reveals that the configuration with the MLP encoder and KAN decoder generally produces
lower errors in velocity and temperature predictions, particularly along the inner cylinder surface, while pressure
predictions remain comparable between the two approaches. Additionally, this configuration achieves a lower minimum
loss (1.25308Eâˆ’4 vs. 4.58812Eâˆ’4), although it requires slightly longer training times per epoch and a marginally
increased number of epochs to converge. The loss evolution for both models is shown in the left panel of Fig. 13.
Accordingly, the loss function of the physics-informed PointNet with an MLP encoder and KAN decoder decreases
to approximately 8.7 Ã— 10âˆ’4 after just 200 epochs, whereas this value remains at 3.8 Ã— 10âˆ’2 at the same epoch for the
model with a KAN encoder and MLP decoder. Additionally, the loss plot of the first model exhibits fewer fluctuations
and appears smoother.
A. Kashefi & T. Mukerji: Preprint submitted to Elsevier
Page 18 of 25
