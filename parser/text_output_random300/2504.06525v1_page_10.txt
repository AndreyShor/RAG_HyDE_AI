 
Figure 4. MOBO on more challenging sample and validation of reward designs | MOBO of 
tapping mode on water droplet sample on mica substrate. Here the drive amplitude, setpoint 
amplitude, and I Gain are chosen to be the controlling parameters. There are 10 initial random 
seeding points and 50 explorative active learning steps in this experiment, and the acquisition 
function of qNEHI is used. A fresh Tap300Al-G is used to optimize scan at 1 Hz in a 5 ùúám area. 
a-b, the training maps of a height and b phase are taken with the different parameters during the 
MOBO process. These maps correspond to the scan lines taken with 1-8th iterations in the MOBO 
trajectory plot in Figure S2a. c, the Pareto scatter plot between the reward of height difference and 
the reward of similarity, which are defined to quantify the same consideration of how well the 
trace is aligned with the retrace scan line. The reward of height difference is better because it shows 
more significant reward improvement compared to the similarity, and it shows smoother change 
in the whole MOBO process which guarantees a larger distinction between good and bad scans. 
d-f, Pareto scatter plot for the rewards of height difference, tip-sample distance, and phase. 
Compared to similar plots in Figure 3, the reward of phase shows better resolution.  
 
VI. 
MOBO as a Human-in-the-Loop Optimization Framework  
MOBO provides a mechanism for incorporating human decision-making into the optimization 
process, particularly in balancing trade-offs between competing rewards along the Pareto front. 
This capability is crucial in scientific experiments where human expertise is needed to prioritize 
specific experimental outcomes. This is particularly useful in the situation where some rewards 
are more important than others, or where it requires human intervention as there is no satisfactory 
optimal solutions under given circumstances.  
