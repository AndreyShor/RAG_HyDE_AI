training data while maintaining consistent or even achieving
better performance.
Prediction MSE Loss
DreamerV3
MoSim
MoSim
w/ noise N ∼(0, 0.012)
Reacher(100 steps)
0.0988
0.0009
0.0008
Panda(100 steps)
0.0971
0.0043
0.0042
Go2(100 steps)
0.4165
0.1282
0.2111
Table 5. Prediction accuracy of MoSim trained on noisy data,
compared to its of DreamerV3 and normal MoSim.
For a self-consistent chaotic system such as Ac-
robot, multiple future trajectories sampling can be reached
through ensemble sampling. Table 6 shows that MoSim’s
Lyapunov characteristic exponent aligns with MuJoCo,
confirming the reliability of ensemble sampling.
3.4. Zero- and Few-Shot Reinforcement Learning
Incorporating MoSim enables us to seamlessly transform
any model-free reinforcement learning algorithm into a
model-based approach, leveraging the predictive capabili-
ties of MoSim’s dynamics model. In this section, we in-
tegrate MoSim with TQC [24] and SAC [25], introducing
three distinct integration strategies, each offering varying
levels of data efficiency.
Zero-Shot Learning. By capitalizing on the robust gener-
alization capabilities and predictive performance of MoSim,
we employed a MoSim model trained on randomized data
as a surrogate for the dm control environment. This enabled
the agent to interact directly with the world model for train-
ing, achieving scores on three dm control benchmark tasks
that closely approximate those of the real environment.
When trying the Cheetah-Run task, two significant chal-
lenges have hindered progress. First, the absolute upper
limit of the model in predictive capability is insufficient. In
the Cheetah-Run task, the MoSim model is able to main-
tain stable predictions only up to approximately 100 steps.
However, it was observed that the existing model-free al-
gorithms fail to successfully learn the Cheetah-Run task
within the original environment when the time limit is set to
100. Additionally, as training progresses, a severe distribu-
tion shift in the training data becomes apparent, eventually
exceeding MoSim’s generalization capacity. At this stage,
due to increasing prediction inaccuracies, the RL training
plateaus (around a score of 100 for the Cheetah-Run task)
and may even begin to degrade. These two issues have im-
peded further progress in MoSim’s zero-shot learning for
the Cheetah-Run task.
Lyapunov Characteristic Exponent (LCE)
δ = 10−5, T = 1000, avg on 2000 trajectories
MuJoCo
MoSim
1.1738
1.1728
Table 6. Numerical results of LCE of MuJoCo and MoSim
Task Name
Cheetah
Reacher-Easy
Reacher-Hard
Acrobot
Horizon
500
30
30
800
Achieved
60
30
30
60
Table 7. Zero-Shot Horizon Requirements. The “Horizon” indi-
cates the minimum number of prediction steps required for com-
plete zero-shot performance, while “Achieved” indicates the cur-
rent number of steps MoSim has successfully reached.
Few-Shot Learning. To address the two challenges en-
countered in zero-shot learning, we propose a few-shot
learning strategy. Instead of solely relying on MoSim to re-
place the virtual environment, we periodically collect real
environment data during training. Specifically, for every
5,000 virtual environment steps, we gather 1,000 steps of
real environment data. This data is then used to train MoSim
by sampling the real dataset every 100 virtual steps, helping
to mitigate the distribution shift that occurs during training.
Additionally, to address the issue of limited prediction hori-
zon, we initialize the virtual environment with states sam-
pled from the real environment replay buffer at each reset,
rather than starting from the initial state every time. This
aims to mimic the effect of having a time limit of 1,000
steps in the real environment.
The results indicate that,
unlike zero-shot learning, where performance plateaus and
then deteriorates, reinforcement learning stabilizes and fluc-
tuates around a score of 100. However, the curve still fails
to improve and remains at a relatively low score.
3.5. How Much Better Do We Need It To Be?
We investigated the minimum environment time limit re-
quired for the original TQC algorithm to achieve its base-
line performance under precise predictions. We varied the
time limit in the dm control environment and evaluated its
impact on the algorithm’s performance. Table 7 implies that
for the TQC algorithm to achieve offline RL without perfor-
mance degradation, the World Model must possess the ca-
pability to continuously predict over the minimum step limit
length across any data distribution. This remains a critical
goal we aim to achieve in the long term.
It is worth noting that the “zero-shot horizon require-
ment” in Table 7 was actually an upper bound, as it was cal-
culated with only one model-free RL algorithm. There ex-
ist algorithms that can make this bound significantly tighter.
For example, MPC requires only a 50-step horizon for the
Acrobot-SwingUp task[26]. We provide a more detailed
discussion of this in the Appendix.
3.6. On-Policy World Model for RL
In previous successful world models, the model was up-
dated in an on-policy and online fashion, where the world
model was trained using samples from a collected replay
buffer at every real environment step. The trained data was
then immediately used as the starting point for the model’s
6
