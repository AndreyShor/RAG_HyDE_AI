gineering. Optimization and Engineering, 12(1):123–144,
2011. 8
[35] Marc P. Deisenroth and Carl E. Rasmussen. Pilco: A model-
based and data-efficient approach to policy search. In Pro-
ceedings of the International Conference on Machine Learn-
ing (ICML), pages 465–472, 2011. 8
[36] Kurtland Chua, Roberto Calandra, Rowan McAllister, and
Sergey Levine. Deep reinforcement learning in a handful
of trials using probabilistic dynamics models. In Advances
in Neural Information Processing Systems (NeurIPS), pages
4754–4765, 2018.
[37] Michael Janner, Justin Fu, Marvin Zhang, and Sergey
Levine. When to trust your model: Model-based policy op-
timization. In Advances in Neural Information Processing
Systems (NeurIPS), pages 12519–12530, 2019.
[38] Evangelos Theodorou, Jonas Buchli, and Stefan Schaal.
Model predictive path integral control: From theory to paral-
lel computation. The International Journal of Robotics Re-
search, 31(2):160–187, 2010. 8
[39] Julian Schrittwieser and et al. Mastering atari, chess, go,
and shogi by planning with a learned model.
Nature,
588(7839):604–609, 2020. 8
[40] David Ha and J¨urgen Schmidhuber. World models. arXiv
preprint arXiv:1803.10122, 2018. 8
[41] Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Ville-
gas, David Ha, Honglak Lee, and James Davidson. Learn-
ing latent dynamics for planning from pixels.
In Inter-
national Conference on Machine Learning (ICML), pages
2555–2565, 2019. 8
[42] Michael J. Azizzadenesheli, Emma Brunskill, and Ani-
mashree Anandkumar. Model-based reinforcement learning
for atari. In International Conference on Learning Represen-
tations (ICLR), 2018. 8
[43] Yifan Xu, Nicklas Hansen, Zirui Wang, Yung-Chieh Chan,
Hao Su, and Zhuowen Tu. On the feasibility of cross-task
transfer with model-based reinforcement learning, 2023.
[44] Pu Hua, Yubei Chen, and Huazhe Xu. Simple emergent ac-
tion representations from multi-task policy training, 2023. 8
[45] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis
Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Lau-
rent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lill-
icrap, Karen Simonyan, and Demis Hassabis. A general rein-
forcement learning algorithm that masters chess, shogi, and
go through self-play. Science, 362(6419):1140–1144, 2018.
8
[46] David Silver, Aja Huang, Chris J. Maddison, Arthur Guez,
Laurent Sifre, George Van Den Driessche, Julian Schrit-
twieser, Ioannis Antonoglou, Veda Panneershelvam, Marc
Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal
Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine
Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Has-
sabis. Mastering the game of go with deep neural networks
and tree search. Nature, 529(7587):484–489, 2016. 8
10
