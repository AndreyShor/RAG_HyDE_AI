Figure 6. (a) Accuracy for one hidden layer with 5 and 11 quantized states. (b) Accuracy for three hidden layers 
with 5 and 11 quantized states. 
 
One partiular distinction is that while we have incorporated quantization and SC for both training (forward pass) 
and inference, [24] and [25] used quantization and SC only for inference.   SC inherently offers resilience to random 
bit flips to some extent, as a single random bit flip in the SC domain has less impact on the system compared to the 
binary domain, especially when the bit flip affects the most significant bit [47].  
 
 
 
 
Energy Consumption:  
Energy consumption for conventional CMOS based LFSR RNG per bit is ~10fJ [25]. For MTJ devices, energy 
dissipation is related to the retention time and the energy barrier Eb between parallel and anti-parallel states. For a 
retention time of τ ≈ 10 ns, the energy per bit is ∼20 fJ assuming an applied voltage of ∼1 V and device resistance of 
500 kΩ, which is comparable to CMOS-only RNGs [24]. With τ < 1 ns, the energy can be reduced further [24].  
We experimented with different bitstreams lengths. In [24] the authors reported 95% accuracy for 1024 bit long stream. 
We achieved 96.44% with 100 bits long stream with three hidden layers or 94.48% with 500 bits for one hidden layer. 
Thus, bitstream generation energy can be lowered from ~2100 nJ to 810~820 nJ for comparable accuracy with 2.6× 
reduction in energy. More importantly, our approach eliminates the requirement of digital to analog converter (DAC) 
which contributes to probably the biggest energy saving.    
MTJ controlling circuitry also consumes energy. To generate bitstreams with deeper resolution in their respective 
probabilities, a complex combination of MTJ bitstreams and additional XNOR operations are required. In our 
proposed approach, bitstreams with only 5 or 11 different probabilities will suffice, which greatly simplifies the control 
circuitry and can be generated by biasing the MTJ with 5 or 11 different voltages.   
Our quantized approach also comes with additional benefit in implementing the neuron activations such as sigmoid 
used in our works. As the neurons and weights assume only a smaller number of discretized states, the sigmoid can 
be implemented using a simple look-up-table (LUT), which will output only a few states. To test the latency 
improvement, we tested sigmoid activation implemented on FPGA using Xilinx software tools. A total of 9 clock 
cycle was needed to compute sigmoid using 11-state LUT while 80 clock cycle was required for similar 32-bit 
precision computation.  
 
V. CONCLUSION 
 
SC-ANN with s-MTJ sourced stochastic bitstream has been tested for MNIST dataset with different bitstream lengths 
and quantized states. It was observed that even with shorter bitstreams of 100 bits and only 5 or 11 quantized states 
accuracy of 92% to over 96% was obtained. This approach achieved 9× improvement in energy and 2.6× improvement 
in latency, making it highly advantageous for neural network inference in edge computing applications. 
 
Acknowledgement 
S.S., W.A. and J. A. acknowledge support from Department of Defense Airforce Grant #FA865123CA023 REQ 
F1TBAX3053A002 grant and National Science Foundation grant #1954589. The work at Northwestern was supported 
by the National Science Foundation under award numbers 1919109, 2322572, and 2311296. 
 
 
