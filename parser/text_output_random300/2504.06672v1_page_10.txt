Jonathan Granskog, and Anastasis Germanidis.
Structure
and content-guided video synthesis with diffusion models.
In ICCV, 2023. 1, 3
[15] Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, Andrew
Tao, Bryan Catanzaro, David Jacobs, Jia-Bin Huang, Ming-
Yu Liu, and Yogesh Balaji. Preserve your own correlation:
A noise prior for video diffusion models. In ICCV, 2023. 2
[16] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel.
Tokenflow: Consistent diffusion features for consistent video
editing. arXiv preprint, 2023. 2
[17] Yuwei Guo, Ceyuan Yang, Anyi Rao, Maneesh Agrawala,
Dahua Lin, and Bo Dai.
Sparsectrl: Adding sparse con-
trols to text-to-video diffusion models.
arXiv preprint
arXiv:2311.16933, 2023. 3
[18] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu
Qiao, Dahua Lin, and Bo Dai. Animatediff: Animate your
personalized text-to-image diffusion models without specific
tuning. arXiv preprint, 2023. 2, 3
[19] Yingqing He, Menghan Xia, Haoxin Chen, Xiaodong Cun,
Yuan Gong, Jinbo Xing, Yong Zhang, Xintao Wang, Chao
Weng, Ying Shan, et al.
Animate-a-story:
Storytelling
with retrieval-augmented video generation. arXiv preprint
arXiv:2307.06940, 2023. 3
[20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-
fusion probabilistic models. Advances in neural information
processing systems, 33:6840–6851, 2020. 3
[21] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,
Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben
Poole, Mohammad Norouzi, David J Fleet, et al. Imagen
video: High definition video generation with diffusion mod-
els. arXiv preprint, 2022. 2
[22] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-
Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.
Lora: Low-rank adaptation of large language models. arXiv
preprint arXiv:2106.09685, 2021. 2
[23] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si,
Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin,
Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin
Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. VBench: Com-
prehensive benchmark suite for video generative models. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition, 2024. 6, 7
[24] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.
Elucidating the design space of diffusion-based generative
models. Advances in neural information processing systems,
35:26565–26577, 2022. 3
[25] Shyamgopal Karthik, Karsten Roth, Massimiliano Mancini,
and Zeynep Akata. If at first you don’t succeed, try, try again:
Faithful diffusion-based text-to-image generation by selec-
tion. arXiv preprint arXiv:2305.13308, 2023. 5
[26] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang,
Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola,
Tim Green, Trevor Back, Paul Natsev, et al. The kinetics hu-
man action video dataset. arXiv preprint arXiv:1705.06950,
2017. 7
[27] Levon Khachatryan, Andranik Movsisyan, Vahram Tade-
vosyan,
Roberto
Henschel,
Zhangyang
Wang,
Shant
Navasardyan, and Humphrey Shi. Text2video-zero: Text-to-
image diffusion models are zero-shot video generators. arXiv
preprint, 2023. 2
[28] PKU-Yuan Lab and Tuzhan AI etc. Open-sora-plan, 2024. 2
[29] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio
Petroni,
Vladimir Karpukhin, Naman Goyal, Heinrich
K¨uttler, Mike Lewis, Wen-tau Yih, Tim Rockt¨aschel, et al.
Retrieval-augmented generation for knowledge-intensive nlp
tasks. Advances in Neural Information Processing Systems,
33:9459–9474, 2020. 1, 3
[30] Lala Li and Ting Chen. Fit: Far-reaching interleaved trans-
formers. 2023. 2
[31] Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya
Jia. Video-p2p: Video editing with cross-attention control.
arXiv preprint, 2023. 2
[32] Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei,
Nan Duan, and Tianrui Li. Clip4clip: An empirical study of
clip for end to end video clip retrieval and captioning. Neu-
rocomputing, 508:293–304, 2022. 4
[33] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Zi-
wei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte:
Latent diffusion transformer for video generation.
arXiv
preprint arXiv:2401.03048, 2024. 2
[34] Yiwei Ma, Guohai Xu, Xiaoshuai Sun, Ming Yan, Ji Zhang,
and Rongrong Ji.
X-clip: End-to-end multi-grained con-
trastive learning for video-text retrieval.
In Proceedings
of the 30th ACM International Conference on Multimedia,
pages 638–647, 2022. 4
[35] Joanna Materzynska, Josef Sivic, Eli Shechtman, Antonio
Torralba, Richard Zhang, and Bryan Russell. Customizing
motion in text-to-video diffusion models.
arXiv preprint
arXiv:2312.04966, 2023. 1, 3
[36] Willi Menapace, Aliaksandr Siarohin, Ivan Skorokhodov,
Ekaterina Deyneka, Tsai-Shien Chen, Anil Kag, Yuwei
Fang, Aleksei Stoliar, Elisa Ricci, Jian Ren, et al.
Snap
video: Scaled spatiotemporal transformers for text-to-video
synthesis.
In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 7038–
7048, 2024. 1, 2
[37] Maxime Oquab, Timoth´ee Darcet, Th´eo Moutakanni, Huy
Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,
Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al.
Dinov2: Learning robust visual features without supervision.
arXiv preprint arXiv:2304.07193, 2023. 6
[38] Elia Peruzzo, Vidit Goel, Dejia Xu, Xingqian Xu, Yifan
Jiang, Zhangyang Wang, Humphrey Shi, and Nicu Sebe.
Vase: Object-centric appearance and shape manipulation of
real videos. arXiv preprint arXiv:2401.02473, 2024. 3
[39] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei,
Xintao Wang, Ying Shan, and Qifeng Chen. Fatezero: Fus-
ing attentions for zero-shot text-based video editing. arXiv
preprint, 2023. 1, 2
[40] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning, pages
8748–8763. PMLR, 2021. 5
10
