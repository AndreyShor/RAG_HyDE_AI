Physics-informed KAN PointNet
global 
feature
Shared KAN (𝑛𝑠× 64, 𝑛𝑠× 64)
Shared KAN (𝑛𝑠× 64, 𝑛𝑠× 128, 𝑛𝑠× 1024)
shared
shared
Shared KAN (𝑛𝑠× 512, 𝑛𝑠× 256, 𝑛𝑠× 128)
𝑛𝑠× 1024
Input
shared
shared
Shared KAN (𝑛𝑠× 128, 𝑛PDE)
𝑁× 2
𝑁× 𝑛𝑠× 64
𝑁× 𝑛𝑠× 1024
𝑁× 𝑛𝑠× 128
𝑁× 𝑛PDE
Output
𝑁× 𝑛𝑠× 1088
Max pool
𝒙1
𝒙2
.
.
.
.
𝒙𝑁
⋯
𝑋1
𝑋2
𝑋𝑚
Spatial coordinates
 of 𝑚 geometries
𝛿
𝛿𝑥
𝛿
𝛿𝑦
𝛿
𝛿𝑥
𝛿
𝛿𝑥
𝛿
𝛿𝑦
𝛿
𝛿𝑦
Automatic
differentiation
Physics-informed
 loss function
𝒙1
𝒙2
.
.
.
.
𝒙𝑁
𝒙1
𝒙2
.
.
.
.
𝒙𝑁
Figure 1: Architecture of the physics-informed Kolmogorov-Arnold PointNet (PI-KAN-PointNet). Shared KANs with the
labels (1, 2) and (1, 2, 3) are explained in the text. 𝑛PDE denotes the number of variables in partial differential
equations. 𝑁is the number of points in the point clouds. 𝑛𝑠is the global scaling parameter used to control the network
size. PI-KAN-PointNet utilizes automatic differentiation to compute spatial derivatives (e.g., 𝛿̃𝑢
𝛿𝑦, 𝛿̃𝑝
𝛿𝑥,
𝛿
𝛿𝑦
(
𝛿̃𝑣
𝛿𝑦
)
, etc.), which
are then used to formulate the physics-informed loss function (see Eq.30).
provided from 𝑖to 𝑖, where 𝑖= {𝒚𝑗∈ℝ𝑛PDE}𝑁
𝑗=1. The variable 𝑛PDE indicates the number of fields we aim to
find. In this study, we focus on predicting the two-dimensional velocity vector, pressure and temperature fields, making
𝑛PDE = 4. Therefore, 𝒚𝑗is the vector of predicted fields at the spatial point 𝒙𝑗, comprising the components 𝑢𝑗, 𝑣𝑗, 𝑝𝑗,
and 𝑇𝑗. Mathematically, this process is described as:
(𝑢𝑗, 𝑣𝑗, 𝑝𝑗, 𝑇𝑗
) = 𝑓((𝑥𝑗, 𝑦𝑗
) , 𝑔(𝑖
)) ;
∀(𝑥𝑗, 𝑦𝑗
) ∈𝑖and ∀(𝑢𝑗, 𝑣𝑗, 𝑝𝑗, 𝑇𝑗
) ∈𝑖with 1 ≤𝑖≤𝑚and 1 ≤𝑗≤𝑁,
(19)
where 𝑓represents the mapping function in PI-KAN-PointNet.
KA-PointNet is designed to be invariant to any permutation of the input vector 𝑖, meaning that if the points in
𝑖are permuted, the geometric structure remains the same, and hence the solution 𝑖should also remain unchanged.
This permutation invariance is achieved using a symmetric function in conjunction with shared KANs. The function
𝑔serves as a symmetric encoder of the geometric features of the point cloud 𝑖. Following the approach introduced in
Ref. [41], we define 𝑔as the maximum function:
𝑔(𝑖
) = max (ℎ(𝑥1, 𝑦1
) , … , ℎ(𝑥𝑁, 𝑦𝑁
)) ;
∀(𝑥𝑗, 𝑦𝑗
) ∈𝑖with 1 ≤𝑖≤𝑚and 1 ≤𝑗≤𝑁,
(20)
where ℎrepresents two shared KAN layers in the first branch of PI-KAN-PointNet (see Fig. 4). In the context of PI-
KAN-PointNet, 𝑔(𝑖
) is referred to as the global feature. The key concept behind PI-KAN-PointNet, as a geometric
deep learning model, is that the predicted fields at each spatial point depend not only on the spatial coordinates of that
point but also on the overall geometric structure of the domain, formed by all points, including the specific point. This
idea is evident in Eqs. (19)–(20).
The batch size, 𝐵, represents the number of point clouds (i.e., 𝑖and 𝑖pairs) processed by KA-PointNet at each
epoch. As illustrated in Fig. 4, the input to KA-PointNet is a three-dimensional tensor of size 𝐵× 𝑁× 2. Following
this, two sequential shared KANs with sizes (64, 64) and (64, 128, 1024) are applied, as shown in Fig. 4. The global
feature, of size 1024, is generated by applying the maximum function. This global feature is then concatenated with an
intermediate feature tensor of size 𝐵× 𝑁× 64, resulting in a tensor of size 𝐵× 𝑁× 1088. Next, two additional shared
KANs are applied, with sizes (512, 256, 128) and (128, 𝑛PDE), respectively, yielding a tensor of size 𝐵× 𝑁× 𝑛PDE, as
depicted in Fig. 4. After each KAN layer (except the last layer in the network), batch normalization [116] is applied to
ensure stable training and avoid divergence of the training loss.
For each 𝑉𝑖(1 ≤𝑖≤𝑚) and its associated pair 𝑖and 𝑖, the residuals of the continuity equation (𝑟continuity
𝑖
),
momentum conservation equations in the 𝑥−direction (𝑟momentum𝑥
𝑖
) and in the 𝑦−directions (𝑟
momentum𝑦
𝑖
), energy con-
servation equation (𝑟energy
𝑖
), along with the residuals of the Dirichlet boundary conditions of the velocity (𝑟velocityBC
𝑖
),
and temperature (𝑟temperatureouter-BC
𝑖
) and sparse observations of the velocity (𝑟velocityobs
𝑖
), pressure (𝑟pressureobs
𝑖
), and
A. Kashefi & T. Mukerji: Preprint submitted to Elsevier
Page 6 of 25
