where p0 ∈[−π, π] × [−π, π] and || ˙p0|| is very small. Thus
the initial state distribution of the system can be defined as
p(x0) ∼U (−xM, xM) , xM =


π
π
ε
ε


(7)
where ε > 0 is a very small constant.
Since the nominal model of the system is available to
develop the controller, we use the forward dynamics function
of the plant as the prior mean function of the change in
velocity for each joint. The available model is
But = M(qt)¨qt + n(qt, ˙qt),
(8)
where M(qt) is the mass matrix, n(qt, ˙qt) contains the
Coriolis, gravitational and damping terms, and B is the
actuation matrix, which is B = diag([1, 0]) for the Pendubot
and B = diag([0, 1]) for the Acrobot. We define then
m∆(˜xt) =
"
m(1)
∆
m(2)
∆
#
:= Ts · M −1(qt)(But −n(qt, ˙qt)) (9)
as the mean function in eq. (4). The control input ut ∈R is a
scalar representing the torque given in input to the controlled
joint. It is important to point out that eq. (9) is nearly perfect
to approximate the system when Ts is sufficiently small,
but it becomes unreliable as Ts grows. In particular, with
Ts = 0.02 s the predictions of eq. (9) are not good enough
to describe the behavior at the unstable equilibrium. The
inaccuracies of the prior mean are compensated by the GP
models. To cope with the large computational burden due to
the high number of collected samples, we implemented the
GP approximation Subset of Data, see [18] for a detailed
description.
An important aspect of policy optimization is the particle
initialization, in this case using the initial distribution eq. (7)
in eq. (6) at the first trial results in a very unreliable
optimization problem, which typically does not converge
to acceptable solutions. For this reason, we employ an
incremental initialization strategy to learn global control for
the system. Namely we use a surrogate initial distribution
for both policy execution and particles initialization:
p′
k(x0) ∼U (−xM · γk, xM · γk) ,
(10)
γk = clip(k −km
K
, 0, 1),
(11)
where k ∈N is the trial index, and km, K ∈N regulate the
increment in the uniform distribution’s bounds. This strategy
falls within Curriculum Learning [19], as the policy is trained
on a task of increasing difficulty.
The cost function must evaluate the policy performance
w.r.t. the task requirements, in this case, we want the system
to reach the position qG = [π, 0]T and stay there indefinitely.
A cost generally used in this kind of system is the saturated
distance from the target state:
cst(xt) = 1 −e−∥|qt|−qG∥2
Σc
Σc = diag
 1
ℓc
, 1
ℓc

,
(12)
0.0
2.5
5.0
7.5
10.0
12.5
15.0
17.5
20.0
k
0.0
0.5
1.0
k
Fig. 1: γk scheduling following eq. (11), with km = 5, K = 10.
0
5000
10000
15000
20000
optimization steps
20
40
60
80
100
total rollout cost
Learning plot
incremental training
standard training
Fig. 2: Total rollout costs in the policy optimization steps of the two MC-
PILCO trainings, the first using the incremental initial distribution, the
second using the nominal initial distribution in all trials.
with ℓc = 3. Notice that this cost does not depend on the
velocity of the system, just on the distance from the goal
state, but it does encourage the policy to reach the goal state
with zero velocity.
The policy function that is used to learn a control strategy
is the general purpose policy from [2]:
πθ(xt) = uM tanh
 Nb
X
i=1
wi
uM
e−∥ai−ϕ(xt)∥2
Σπ
!
ϕ(xt) = [ ˙qT
t , cos (qT
t ), sin (qT
t )]T
(13)
with hyperparameters θ = {w, A, Σπ}, where w =
[w1, . . . , wNb]T and A = {a1, . . . , aNb} are, respectively,
weights and centers of the Nb Gaussians basis functions,
whose shapes are determined by Σπ. For both robots, the
dimensions of the elements of the policy are: Σπ ∈R6×6,
ai ∈R6, wi ∈R for i = 1, . . . , Nb, since the policy
outputs a single scalar. In the experiments, the parameters
are initialized as follows. The basis weights are sampled
uniformly in [−uM, uM], the centers are sampled uniformly
in the image of ϕ with ˙qt ∈[−2π, 2π] rad/s. The matrix Σπ
is initialized to the identity.
IV. EXPERIMENTS
In this section, we briefly discuss how the algorithm was
applied to both systems and show the main results. We also
report the optimization parameters used for both systems, all
the parameters not specified are set to the values reported
in [2]. All the code was implemented in Python with the
PyTorch [20] library.
