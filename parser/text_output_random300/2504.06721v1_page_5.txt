V. CONCLUSIONS
In both systems, our MBRL approach is able to solve the
global task with good swing-up time, handling the uniform
initial state distribution.
ACKNOWLEDGEMENTS
Alberto Dalla Libera and Giulio Giacomuzzo were sup-
ported by PNRR research activities of the consortium iNEST
(Interconnected North-Est Innovation Ecosystem) funded by
the European Union Next GenerationEU (Piano Nazionale di
Ripresa e Resilienza (PNRR) – Missione 4 Componente 2,
Investimento 1.5 – D.D. 1058 23/06/2022, ECS 00000043).
This manuscript reflects only the Authors’ views and opin-
ions, neither the European Union nor the European Commis-
sion can be considered responsible for them.
REFERENCES
[1] F. Wiebe, S. Vyas, L. J. Maywald, S. Kumar, and F. Kirchner,
“Realaigym: Education and research platform for studying athletic
intelligence,” in Proceedings of Robotics Science and Systems Work-
shop Mind the Gap: Opportunities and Challenges in the Transition
Between Research and Industry, New York, 2022.
[2] F. Amadio, A. Dalla Libera, R. Antonello, D. Nikovski, R. Carli,
and D. Romeres, “Model-based policy search using monte carlo
gradient estimation with real systems application,” IEEE Transactions
on Robotics, vol. 38, no. 6, pp. 3879–3898, 2022.
[3] F. Wiebe, N. Turcato, A. Dalla Libera, C. Zhang, T. Vincent, S. Vyas,
G. Giacomuzzo, R. Carli, D. Romeres, A. Sathuluri, M. Zimmermann,
B. Belousov, J. Peters, F. Kirchner, and S. Kumar, “Reinforcement
learning for athletic intelligence: Lessons from the 1st “ai olympics
with realaigym” competition,” in Proceedings of the Thirty-Third
International Joint Conference on Artificial Intelligence, IJCAI-24
(K. Larson, ed.), pp. 8833–8837, International Joint Conferences on
Artificial Intelligence Organization, 8 2024. Demo Track.
[4] F. Wiebe, N. Turcato, A. Dalla Libera, J. S. B. Choe, B. Choi,
T. L. Faust, H. Maraqten, E. Aghadavoodi, M. Cali, A. Sinigaglia,
et al., “Reinforcement learning for robust athletic intelligence: Lessons
from the 2nd’ai olympics with realaigym’competition,” arXiv preprint
arXiv:2503.15290, 2025.
[5] N.
Turcato,
A.
Dalla
Libera,
G.
Giacomuzzo,
R.
Carli,
and
D. Romeres, “Learning control of underactuated double pendulum
with model-based reinforcement learning,” 2024.
[6] F. Amadio, A. Dalla Libera, D. Nikovski, R. Carli, and D. Romeres,
“Learning control from raw position measurements,” in 2023 American
Control Conference (ACC), pp. 2171–2178, IEEE, 2023.
[7] N. Turcato, A. Dalla Libera, G. Giacomuzzo, and R. Carli, “Teaching
a robot to toss arbitrary objects with model-based reinforcement
learning,” in 2023 9th International Conference on Control, Decision
and Information Technologies (CoDIT), pp. 1126–1131, 2023.
[8] N. Turcato, G. Giacomuzzo, M. Terreran, D. Allegro, R. Carli, and
A. Dalla Libera, “Data efficient robotic object throwing with model-
based reinforcement learning,” arXiv preprint arXiv:2502.05595,
2025.
[9] Z. Liang, K. Zhao, J. Xie, and Z. Zhang, “Ultra-fast tuning of neural
network controllers with application in path tracking of autonomous
vehicle,” ISA Transactions, vol. 149, pp. 394–408, 2024.
[10] F. Wiebe, S. Kumar, L. J. Shala, S. Vyas, M. Javadi, and F. Kirchner,
“Open source dual-purpose acrobot and pendubot platform: Bench-
marking control algorithms for underactuated robotics,” IEEE Robotics
& Automation Magazine, vol. 31, no. 2, pp. 113–124, 2024.
[11] C. E. Rasmussen, “Gaussian processes in machine learning,” in
Summer school on machine learning, pp. 63–71, Springer, 2003.
[12] A. Car`e, R. Carli, A. Dalla Libera, D. Romeres, and G. Pillonetto,
“Kernel methods and gaussian processes for system identification and
control: A road map on regularized kernel-based learning for control,”
IEEE Control Systems Magazine, vol. 43, no. 5, pp. 69–110, 2023.
[13] R. E. Caflisch, “Monte carlo and quasi-monte carlo methods,” Acta
numerica, vol. 7, pp. 1–49, 1998.
[14] L. Bottou, “Large-scale machine learning with stochastic gradient
descent,” in Proc of COMPSTAT’2010, pp. 177–186, Springer, 2010.
[15] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,”
arXiv preprint arXiv:1312.6114, 2013.
[16] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhut-
dinov, “Dropout: a simple way to prevent neural networks from
overfitting,” JMLR, vol. 15, no. 1, pp. 1929–1958, 2014.
[17] F. Amadio, A. Dalla Libera, D. Nikovski, R. Carli, and D. Romeres,
“Learning control from raw position measurements,” 2023.
[18] J. Qui˜nonero-Candela and C. E. Rasmussen, “A unifying view of
sparse approximate gaussian process regression,” Journal of Machine
Learning Research, vol. 6, no. 65, pp. 1939–1959, 2005.
[19] X. Wang, Y. Chen, and W. Zhu, “A survey on curriculum learning,”
IEEE Transactions on Pattern Analysis and Machine Intelligence,
vol. 44, no. 9, pp. 4555–4576, 2022.
[20] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito,
Z. Lin, A. Desmaison, L. Antiga, and A. Lerer, “Automatic differen-
tiation in pytorch,” 2017.
