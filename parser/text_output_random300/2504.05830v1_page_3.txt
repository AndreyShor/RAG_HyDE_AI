Springer Nature 2021 LATEX template
Human Activity Recognition using RGB-Event based Sensors
3
Fig. 1 (a). Comparison between existing datasets and our proposed HARDVS 2.0 dataset
for video classification. (b). A simple schematic diagram of our framework.
Recently, bio-inspired sensors (also termed event-based cameras), such as
DAVIS [10], CeleX-V [11], ATIS [12], and PROPHESEE 1, drawing more and
more researchers’ attention and have been introduced for research on pattern
recognition, object detection and tracking [13–18]. Unlike the synchronous
optical imaging principle of RGB cameras, event cameras record event signals
generated by changes in illumination caused by the movement of objects in an
asynchronous manner. Specifically, each pixel in the field of view of the event
cameras independently records a binary signal generated by lighting increases
or decreases (if and only if a certain threshold is exceeded), which is commonly
referred to as polarity. Due to the unique imaging principle of event cameras,
they typically offer sparse spatial resolution and dense temporal resolution. As
a result, event cameras tend to provide stable imaging when capturing rapidly
changing human movements, such as the fast hand movements of magicians
and the swift sports activities of an athlete, without causing motion blur. Addi-
tionally, since event signals are generated by changes in illumination, the event
cameras do not require strict lighting conditions, exhibiting a high dynamic
range (HDR) advantage. Thus, they can perform well in overexposed and low-
light scenarios. In light of the aforementioned issues with RGB cameras and
the benefits of the unique advantages of event cameras, we consider combining
event cameras with RGB cameras to address the challenges in human activity
recognition tasks.
Since this research direction is still in its early stages, there are not many
open-source public datasets available in academia. Although several bench-
mark datasets have been proposed for classification tasks [19–29], most of them
are simulated or synthetic datasets derived from RGB videos using simulators.
Some researchers also obtain event data by recording screens while displaying
RGB videos. However, these datasets fail to capture the real-world character-
istics of event cameras, particularly in fast-motion and low-light scenarios. The
ASL-DVS dataset [19] proposed by Bi et al. consists of 100, 800 samples but
is limited to hand gesture recognition with only 24 classes. DvsGesture [20]
is also constrained in scale and category coverage, which limits its relevance
in the context of deep learning. Furthermore, some datasets, like DvsGesture,
1https://www.prophesee.ai
