of the maximums of reward 1 and 2 (blue point in Figure S1h), showing no preference on neither 
reward 1 nor reward 2. When we lower the reference point for reward 1 (the orange point in Figure 
S1e), the new optimal solution given by MOBO shifts away from the maximum of this reward 
along the Pareto front, indicated by the orange point in Figure S1h. Therefore, to favor a reward, 
one needs to increase the corresponding reference point closer to the maximum of that reward. 
The other method is to multiply weights directly to the measured rewards before feeding them 
to the acquisition function. For example, in Figure S1i, we show the results of multiplying the 
reward 1 with weights ranging from 0.75 to 1.25 on the optimal solutions predicted by the same 
MOBO. When a weight of greater than 1 is multiplied to a reward, the optimal solution will be 
moved away from its maximum along the Pareto front. In other words, adding a larger weight to a 
reward will suppress it in the MOBO. 
 
 
Figure S1. Overview of MOBO and Pareto front | a, an example distribution of reward in the 
parameter space. b, reward 2 is 90-degree rotated counter-clockwise with respect to reward 1 so 
