[43] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing
Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Mea-
suring multimodal mathematical reasoning with math-vision
dataset. Advances in Neural Information Processing Systems,
37:95095–95169, 2025. 1
[44] Weiyun Wang, Zhe Chen, Wenhai Wang, Yue Cao, Yangzhou
Liu, Zhangwei Gao, Jinguo Zhu, Xizhou Zhu, Lewei Lu,
Yu Qiao, and Jifeng Dai. Enhancing the reasoning ability
of multimodal large language models via mixed preference
optimization. arXiv preprint arXiv:2411.10442, 2024. 7
[45] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma,
Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-
thought prompting elicits reasoning in large language models.
Advances in neural information processing systems, 35:24824–
24837, 2022. 2
[46] Peng Xia, Ze Chen, Juanxi Tian, Gong Yangrui, Ruibo Hou,
Yue Xu, Zhenbang Wu, Zhiyuan Fan, Yiyang Zhou, Kangyu
Zhu, et al. Cares: A comprehensive benchmark of trustwor-
thiness in medical vision language models. In The Thirty-
eight Conference on Neural Information Processing Systems
Datasets and Benchmarks Track, 2025. 1
[47] Shuai Yang, Yuying Ge, Yang Li, Yukang Chen, Yixiao Ge,
Ying Shan, and Yingcong Chen. Seed-story: Multimodal long
story generation with large language model. arXiv preprint
arXiv:2407.08683, 2024. 1
[48] Yue Yang, Shuibai Zhang, Wenqi Shao, Kaipeng Zhang, Yi
Bin, Yu Wang, and Ping Luo. Dynamic multimodal evaluation
with flexible complexity by vision-language bootstrapping.
arXiv preprint arXiv:2410.08695, 2024. 3
[49] Kaining Ying, Fanqing Meng, Jin Wang, Zhiqian Li, Han Lin,
Yue Yang, Hao Zhang, Wenbo Zhang, Yuqi Lin, Shuo Liu,
et al. Mmt-bench: A comprehensive multimodal benchmark
for evaluating large vision-language models towards multi-
task agi. In Forty-first International Conference on Machine
Learning. 1, 2
[50] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi
Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming
Ren, Yuxuan Sun, et al. MMMU: A massive multi-discipline
multimodal understanding and reasoning benchmark for ex-
pert AGI. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 9556–9567,
2024. 1, 2, 3
[51] Eric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri,
Nick Haber, and Noah D Goodman. Quiet-STaR: Language
models can teach themselves to think before speaking. arXiv
preprint arXiv:2403.09629, 2024. 2
[52] Eric Zelikman, YH Wu, Jesse Mu, and Noah D Goodman.
Star: Self-taught reasoner bootstrapping reasoning with rea-
soning. In Proc. the 36th International Conference on Neural
Information Processing Systems, 2024. 2
[53] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu
Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang,
Yu Qiao, et al. Mathverse: Does your multi-modal llm truly
see the diagrams in visual math problems?
In European
Conference on Computer Vision, pages 169–186. Springer,
2024. 2, 3
[54] Han Zhong, Guhao Feng, Wei Xiong, Xinle Cheng, Li
Zhao, Di He, Jiang Bian, and Liwei Wang.
Dpo meets
ppo: Reinforced token optimization for rlhf. arXiv preprint
arXiv:2404.18922, 2024. 2
[55] Pengfei Zhou, Xiaopeng Peng, Jiajun Song, Chuanhao Li,
Zhaopan Xu, Yue Yang, Ziyao Guo, Hao Zhang, Yuqi Lin,
Yefei He, et al. Gate opening: A comprehensive benchmark
for judging open-ended interleaved image-text generation.
arXiv preprint arXiv:2411.18499, 2024. 2
[56] Kaijie Zhu, Jindong Wang, Qinlin Zhao, Ruochen Xu, and
Xing Xie. Dynamic evaluation of large language models by
meta probing agents. In Forty-first International Conference
on Machine Learning, 2024. 3
