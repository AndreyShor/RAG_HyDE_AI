Figure 5. Breakdown of accuracy on MDK12-Mini across different exam years.
Figure 6. Accuracy of MLLMs on full-set of MDK12-Bench. We
demonstrate the results across six disciplines (mathematics, physics,
chemistry, biology, geography, and information science) and three
grade levels (elementary, middle school, and high school).
5. Experiments
5.1. Baselines and Setup
We evaluate a set of both closed-source and open-
source MLLMs, including: 1) Closed-source MLLMs:
Gemini-2.0-flash-exp [40] (Gemini2-flash), Gemini-2.0-
flash-thinking-exp [40] (Gemini2-thinking), GPT-4o [29],
GPT-o1-mini [30], GPT-o3-mini [31], Claude-3.7-Sonnet
(Claude-3.7) [3]. 2) Open-source MLLMs: Qwen2.5-VL
[4], InternVL2.5 [8], QVQ-72B-preview (QVQ-72B) [41],
InternVL2.5-78B-MPO (InternVL2.5-MPO) [44], etc. 3)
Reasoning-oriented R1 variants: Qwen2-VL-7B-GRPO-
8K [11]. We mainly test the largest available models with
certain reasoning capabilities, with smaller ones included
for comparison if resources allow. All models are evaluated
on three difficulty subsets (easy, medium, and hard). Further
technical details will be discussed in the supplement.
Figure 7. Test logic of using subsets and the fullset data of
MDK12-Bench progressively.
5.2. Results on MDK12-mini
We present the performance of baselines across easy,
medium, and hard subsets in Table 3. Gemini2-thinking
achieves the highest overall accuracy of 59.4%, notably
excelling in Chemistry and Biology. Gemini2-flash fol-
lows closely with an overall accuracy of 57.2%, surpass-
ing Claude-3.7 and GPT-based series. QVQ-72B achieves
the best performance (53.2% overall accuracy) among open-
source models, showing superior reasoning performance.
InternVL2.5-MPO shows exceptional results in Biology at
medium difficulty, though performance in other disciplines
is mixed. InternVL2.5-8B consistently underperforms com-
pared with other models. Overall, larger models generally
perform better, while variability across different subjects is
also observed, implying varied domain challenges. These
results highlight the necessity for targeted improvements in
discipline-specific multimodal reasoning.
Fig. 4 presents the accuracy distribution with respect to
specific knowledge points, aggregated across all subjects.
Each question is annotated with one or more fine-grained
knowledge labels. We highlight that models consistently
achieve higher accuracy on frequently covered knowledge
points in the training data, while systematically underper-
forming in areas such as advanced geometry and biochemical
processes. The results highlight that our benchmark effec-
tively helps identify specific knowledge gaps within models,
enabling targeted in-depth evaluation of the full dataset and
facilitating focused improvements in these weaker knowl-
edge areas. Furthermore, Fig. 5 provides a year-by-year
