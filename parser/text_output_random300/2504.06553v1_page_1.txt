ASHiTA: Automatic Scene-grounded HIerarchical Task Analysis
Yun Chang*
Leonor Fermoselle†
Duy Ta†
Bernadette Bucher‡
Luca Carlone*
Jiuguang Wang†
Abstract
While recent work in scene reconstruction and understand-
ing has made strides in grounding natural language to phys-
ical 3D environments, it is still challenging to ground ab-
stract, high-level instructions to a 3D scene.
High-level
instructions might not explicitly invoke semantic elements
in the scene, and even the process of breaking a high-level
task into a set of more concrete subtasks —a process called
hierarchical task analysis— is environment-dependent. In
this work, we propose ASHiTA, the first framework that
generates a task hierarchy grounded to a 3D scene graph
by breaking down high-level tasks into grounded subtasks.
ASHiTA alternates LLM-assisted hierarchical task analy-
sis —to generate the task breakdown— with task-driven 3D
scene graph construction to generate a suitable representa-
tion of the environment. Our experiments show that ASHiTA
performs significantly better than LLM baselines in break-
ing down high-level tasks into environment-dependent sub-
tasks and is additionally able to achieve grounding perfor-
mance comparable to state-of-the-art methods.
1. Introduction
Modern machine vision applications, ranging from robotics
to augmented reality, demand the development of vision
systems that are able to parse and support the execution
of high-level instructions, possibly provided by non-expert
users. For example, consider a robot that is given a high-
level task of preparing for dinner. To carry out this task,
the robot is required to decompose the task into granular
subtasks like fetching objects or visiting locations of inter-
est (e.g., go to the kitchen, preheat the oven). Moreover, it
must be able to ground these instructions into 3D objects
and locations observed in the environment.
Towards these goals, prior work has mostly focused
on developing more expressive map representations, which
*LIDS,
Massachusetts
Institute
of
Technology,
MA,
USA.
{yunchang, lcarlone}@mit.edu
†Robotics and AI Institute, MA, USA. {lfermoselle, dta,
jw}@theaiinstitute.com
‡Department of Robotics,
University of Michigan,
MI, USA.
bucherb@umich.edu
This work was done partially while Chang was an intern at the Boston Dy-
namics AI Institute and partially supported by the ARL DCIST Program.
Prepare for dinner
Set the dining table.
Preheat the oven if you’re cooking.
Wash the plates and utensils thoroughly.
Refrigerate 
beverages and 
chilled food.
Plate
Dining Table
Oven
Sink
Stove
Refrigerator
Figure 1.
Given the high-level task of "Prepare for dinner", ASHiTA au-
tomatically generates a hierarchy of subtasks (and items) while grounding
them to a 3D scene graph. For the scene graph in the figure, the blue node
corresponds to the high-level task, magenta nodes correspond to the sub-
tasks, and red nodes correspond to the items required by the subtasks.
combines both geometric and semantic information, to fa-
cilitate the grounding process. Related work investigates
sparse object-based metric-semantic maps [36, 38], dense
metric-semantic maps [31, 35], and 3D scene graphs [2,
15, 46].
Traditionally, these representations have relied
on closed-set semantic labels, where the semantic concepts
are restricted to the set of labels annotated in the training
dataset. However, using a pre-defined set of labels limits the
expressiveness of the map model (e.g., if our robot only has
a “chair” class, it might not be able to distinguish a folding
chair from a rocking chair), thus imposing constraints and
creating ambiguity when grounding language commands.
To overcome this constraint, recent work has utilized
new foundation models with open-set semantics for map-
ping [12, 17, 45]. However, as pointed out in [30], when
using open-set semantics, it becomes unclear how to group
objects (e.g., is the zipper of a bag an object, or only
the bag itself?) Therefore, the work [30] proposes an ap-
proach that clusters open-set concepts in the map in a task-
driven fashion, thus representing the objects and places in a
scene graph at the granularity required by specified natural-
language tasks. However, the resulting approach, Clio [30],
still cannot support high-level tasks and is limited to sup-
porting simple tasks that explicitly invoke relevant objects
and locations (e.g., preheat the oven, go to the kitchen).
arXiv:2504.06553v1  [cs.RO]  9 Apr 2025
