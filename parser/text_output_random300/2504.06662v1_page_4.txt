Fig. 3: Overview of the RAMBO framework.
In addition, categorizing the end-effectors i ∈N into
two kinds: locomotion L ⊂N, marked by superscript l,
and manipulation M ⊂N, marked by superscript m, our
kinematic reference generation also accept specification for
the desired EE positions P ˆpm
i ∈R3 for the limbs responsible
for manipulation. By incorporating predefined gait patterns
and their contact schedules, P ˆpl
i is generated by interpolating
the keyframes (P plift, P pmid, P pland)i as shown in Section III-
B. The reference for each joint ˆqj is calculated using inverse
kinematics (IK)
ˆqj = IK(P p, P ˆp1, ..., P ˆpN).
(6)
For accounting the dynamics effect from manipulation
tasks, the contact state for manipulation ˆcm = {ˆci|i ∈M}
is always set to 1. The remaining contact state of locomotion
end-effector ˆcl = {ˆci|i ∈L} is predefined according to the
gait pattern. We fill unspecified references with either current
state or zeros
P ˆp = P p, P ˆR = P R
P ˆv = [P ˆvx, P ˆvy, 0], P ˆω = [0, 0, P ˆωz], ˆ˙qj = 0.
(7)
B. Generating Feedforward Torque
To account for contributions from all contacts and ensure
the robot tracks the reference motion, we employ a com-
putationally lightweight reaction force optimization module
leveraging the single rigid body model.
We firstly calculate the base linear and angular acceler-
ation target P ˆa = (P ˆal, P ˆaa) ∈R6 using a proportional-
derivative controller
P ˆal = κlp(P ˆp −P p) + κld(P ˆv −P v)
(8)
P ˆaa = κaplog(P ˆR⊤· P R)∨+ κad(P ˆω −P ω),
(9)
where κlp, κld, κap, κad are the linear and angular, propor-
tional and derivative gains respectively; log(·)∨: SO(3) →
R3 converts a rotation matrix into an angle-axis representa-
tion, which is a vector in R3.
In addition to the desired acceleration P a, RAMBO also
accept user’s specification of desired EE forces P ˆui for the
manipulation end-effectors i ∈M. By converting to the base
frame B and using Ba and B ˆui as targets, we formulate the
following QP to optimize the reaction force
min
Bui,i∈N
∆Ba
2
U +
X
i∈M
∆Bui
2
V +
X
i∈L
Bui
2
W (10a)
subject to : Ba =
N
X
i=1
Ai
Bui + ˆg
(10b)
(Bui)z = 0,
i ∈Lswing
(10c)
(Bui)x
 ≤µ(Bui)z,
i ∈Lstance
(10d)
(Bui)y
 ≤µ(Bui)z,
i ∈Lstance
(10e)
uz,min ≤(Bui)z ≤uz,max,
i ∈Lstance
(10f)
where ∆Ba =
B ˆa −Ba, ∆Bui =
B ˆui −Bui; Ba =
[B ˙v, B ˙ω] ∈R6 is the acceleration of the single rigid body;
Ai is the generalized inverse inertia matrix defined in Eq. 4;
µ is the friction coefficient; Lswing, Lstance are the set of end-
effectors for locomotion in swing and stance respectively;
U, V , W ≻0 are positive definite weight matrices. We note
that friction checking is only performed on locomotion end-
effectors due to the uncertainty of contact surface for ma-
nipulation. Leveraging the SRB model, RAMBO efficiently
accounts for the dynamic effects from contacts in loco-
manipulation.
The feedforward joint torques τ FF
j
are calculated using
τ FF
j
=
N
X
i=1
J⊤
i · Bui,
(11)
where Ji is the Jacobian corresponds to the end-effector i.
In addition to the feedforward torque from the reaction
force optimization module, we calculate an additional torque
term τ GC
j
to compensate the gravity and account for the limb
inertia, for each joint k
(τ GC
j
)k = −
X
l∈D(k)
J⊤
lk · mlg,
(12)
where Jlk is the Jacobian matrix mapped from the CoM
velocity of link l to joint k; ml is the mass of link l, and
D(k) is a set of descendant links of k.
C. Learned Residual Policy
Directly applying the feedforward torque τj may not
be enough to accomplish complex loco-manipulation tasks
due to the large model mismatch. As a remedy, RAMBO
incorporate a learned residual policy trained in simulated
environments using Reinforcement Learning to improve the
overall robustness of the controller over unconsidered dy-
namic effects.
An RL problem is formulated as a Markov Decision
Process (MDP), represented by a tuple ⟨O, A, P, r, γ⟩, where
O is the observation space; A is the action space; PO′|O,A
is the transition probability; r : O × A →R is the reward
function; γ is the discounted factor. The residual policy
