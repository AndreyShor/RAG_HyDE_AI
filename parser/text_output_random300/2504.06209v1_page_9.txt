9
[24] Y. Ephraim and N. Merhav, Hidden Markov processes,
IEEE Transactions on information theory 48, 1518
(2002).
[25] R. G. Gallager, Information theory and reliable commu-
nication, Vol. 588 (Springer, 1968).
[26] R. B. Ash, Basic Probability Theory (Courier Corpora-
tion, 2008).
[27] M. Iosifescu, Finite Markov Processes and Their Appli-
cations (Courier Corporation, 2014).
[28] For example, eq. (9) is satisfied if eq. (8) holds true for
all times, or it can be satisfied when the summands in
eq. (9) decay sufficiently quickly.
[29] T. M. Cover and J. A. Thomas, Elements of Information
Theory (Wiley, 2005).
[30] N. Barnett and J. P. Crutchfield, Computational me-
chanics of input–output processes: Structured transfor-
mations and the epsilon-transducer, J. Stat. Phys. 161,
404 (2015).
[31] R. B. Ash, Information Theory, Interscience Tracts in
Pure and Applied Mathematics No. 19 (John Wiley &
Sons).
[32] Technically, this also requires allowing for an infinite
number of hidden states in the environment to generate
all percept processes allowed in the tape setting.
[33] R. M. Gray, Probability, random processes, and ergodic
properties, Vol. 1 (Springer, 2009).
[34] S. Still, D. A. Sivak, A. J. Bell, and G. E. Crooks, Ther-
modynamics of Prediction, PRL 109, 120604 (2012).
[35] In fact, the other condition is I[S0:t; Mt | St:∞] = 0,
which corresponds to a d-separation in the Bayesian net-
work underlying the percept–action loop (see Supplemen-
tal Material E for details on d-separation).
[36] R. Landauer, Information is Physical, Physics Today 44,
23 (1991).
[37] S. Deffner and C. Jarzynski, Information Processing
and the Second Law of Thermodynamics: An Inclusive,
Hamiltonian Approach, PRX 3, 041003 (2013).
[38] A. Kolchinsky and D. H. Wolpert, Dependence of dis-
sipation on the initial distribution over states, Journal
of Statistical Mechanics: Theory and Experiment 2017,
083202 (2017).
[39] N. Shiraishi, K. Funo, and K. Saito, Speed Limit for Clas-
sical Stochastic Processes, PRL 121, 070601 (2018).
[40] A. Kolchinsky and D. H. Wolpert, Work, Entropy Pro-
duction, and Thermodynamics of Information under Pro-
tocol Constraints, PRX 11, 041024 (2021).
[41] D. H. Wolpert, J. Korbel, C. W. Lynn, F. Tasnim, J. A.
Grochow, G. Karde¸s, J. B. Aimone, V. Balasubramanian,
E. De Giuli, D. Doty, et al., Is stochastic thermodynamics
the key to understanding the energy costs of computa-
tion?, Proceedings of the National Academy of Sciences
121, e2321112121 (2024).
[42] An illustrative example of a sequence (at)t where the
Ces´aro limit ⟨at⟩t fails to exist is 0110000 . . . , where one
zero is followed by twice as many ones, followed again
by twice as many zeros, and so on. This results in an
oscillating arithmetic mean 1/n Pn
t=1 at as n →∞.
[43] The term noiseless channel is inherited from communica-
tion theory, where it refers to an ideal channel that trans-
mits input symbols without alteration—that is, without
introducing noise.
[44] C. E. Shannon, A mathematical theory of communica-
tion, The Bell system technical journal 27, 379 (1948).
[45] A. B. Boyd, D. Mandal, and J. P. Crutchfield, Identifying
functional thermodynamics in autonomous Maxwellian
ratchets, NJP 18, 023049 (2016).
[46] If the percept process is a stationary finite-state Markov
chain, a closed-form expression for the entropy rate exists
[44].
[47] T. Parr, G. Pezzulo, and K. J. Friston, Active Inference:
The Free Energy Principle in Mind, Brain, and Behavior
(MIT Press, 2022).
[48] D. T. Larose, Data Mining and Predictive Analytics
(John Wiley & Sons, 2015).
[49] J. P. Crutchfield, Between order and chaos, Nature
Physics 8, 17 (2012).
[50] S. Boccaletti, C. Grebogi, Y.-C. Lai, H. Mancini, and
D. Maza, The control of chaos: theory and applications,
Physics reports 329, 103 (2000).
[51] T. Lin, Y. Wang, X. Liu, and X. Qiu, A survey of trans-
formers, AI open 3, 111 (2022).
[52] M. Navascu´es and L. P. Garc´ıa-Pintos, Nonthermal
Quantum Channels as a Thermodynamical Resource,
PRL 115, 010405 (2015).
[53] P. Faist, M. Berta, and F. Brand˜ao, Thermodynamic Ca-
pacity of Quantum Processes, PRL 122, 200601 (2019).
[54] M. Gu, K. Wiesner, E. Rieper, and V. Vedral, Quantum
mechanics can reduce the complexity of classical models,
Nat. Commun. 3, 762 (2012).
[55] V. Dunjko, J. M. Taylor, and H. J. Briegel, Quantum-
enhanced machine learning, PRL 117, 130501 (2016).
[56] A. Rupe and J. P. Crutchfield, On principles of emergent
organization, Physics Reports 1071, 1 (2024).
[57] K. Friston, Life as we know it, Journal of the Royal So-
ciety Interface 10, 20130475 (2013).
[58] A. S. Klyubin, D. Polani, and C. L. Nehaniv, Represen-
tations of Space and Time in the Maximization of In-
formation Flow in the Perception-Action Loop, Neural
computation 19, 2387 (2007).
[59] Note that the labeling convention is such that the value
left of the colon in the subscript is included in the se-
quence, while the value to the right is not.
[60] R. M. Gray, Entropy and information theory, first, cor-
rected ed. (Springer Science & Business Media, 2011).
[61] D. Hankerson, G. A. Harris, and P. D. Johnson Jr, In-
troduction to Information Theory and Data Compression
(CRC press, 2003).
[62] R. W. Yeung, A First Course in Information Theory
(Springer US, 2002).
[63] A. Kolchinsky, A novel approach to the partial informa-
tion decomposition, Entropy 24, 403 (2022).
[64] R. W. Yeung, A new outlook on Shannon’s information
measures, IEEE transactions on information theory 37,
466 (1991).
[65] H. K. Ting, On the Amount of Information, Theory of
Probability & Its Applications 7, 439 (1962).
[66] H. C. Tijms, A First Course in Stochastic Models (John
Wiley and sons, 2003).
[67] R. G. Gallager, Discrete Stochastic Processes (Springer,
1996).
[68] R. E. Edwards, Fourier Series: A Modern Introduction
Volume 1, 2nd ed. (Springer, 1979).
[69] J. R. Munkres, Topology:
Pearson New International
Edition (Pearson Higher Ed, 2013).
[70] J. C. Kieffer and M. Rahe, Markov Channels are Asymp-
totically Mean Stationary, SIAM Journal on Mathemat-
ical Analysis 12, 293 (1981).
[71] R. G. James, J. R. Mahoney, and J. P. Crutchfield, In-
