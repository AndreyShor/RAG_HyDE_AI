Figure 4. Ablation study of inductive bias on Hopper-Hop.
Figure 5. Ablation study of training method on Hopper-Hop.
Figure 6. Policy learning with different prediction horizen
3.9. Ablation Study
We conducted two ablation studies to evaluate the effective-
ness of the network architecture and training strategies dis-
cussed in Section 2.
Inductive Bias of Rigid Body Dynamics: We compared
the predictor model with a standard ResNet of nearly iden-
tical parameter count to demonstrate the necessity of intro-
ducing inductive bias. The results, shown in Figure 4, illus-
trate the clear benefits of inductive bias in terms of training
speed and overall performance.
Multistage Training: We conducted an ablation study us-
ing a predictor model and a corrector model with identi-
cal parameter counts. Both models were evaluated under
full training and multi-stage training regimes. The results,
presented in Figure 5, demonstrate the effectiveness of the
multi-stage training approach.
4. Additional Related Works
Differentiable Simulation. Recent differentiable physics
engines [27, 28] offer end-to-end gradients via hand-coded
rigid/soft-body models.
In contrast, our data-driven ap-
proach minimizes manual force/contact design, using a
structural predictor with trainable correctors.
Neural ODE Usage. Neural ODEs have been applied to
time-series modeling [29], fluid simulation [30], system
ID [31], and generative modeling [32], and have inspired
physics-based networks [33]. Building on this, we use a
neural ODE backbone with rigid-body insight and residual
correctors for robust long-horizon prediction in robotic con-
trol.
Parameter Identification.
Classical optimal control re-
fines simulations via identifying parameters like mass or
friction [34]. In contrast, our method learns a scalable dy-
namics model from data, capturing smooth and non-smooth
effects without manual engineering.
World Models and Model-Based RL. One major applica-
tion of Existing world models are mainly used in reinforce-
ment learning, where they predict future states in explicit
or latent space as action-conditioned models [6–9, 35–39].
Such inner-model-based methods are called model-based
RL and offer key advantages over model-free approaches
by leveraging model predictions.
1. Pixel-Based Video Generation World Models.
An-
other line of work predicts future frames from actions
for pixel-based policy learning [40, 41].
Models like
DreamerV2/V3 [9] learn latent visual representations but
struggle with fine-grained physical interactions like con-
tact.
2. Data Efficient Learning: RL algorithms can learn from
the data generated by a trained model’s predictions (or
“dreams”), effectively treating the model as a form of
data augmenter.[35, 42–44]
3. Planning Capability: Many real-world tasks demand
long-term planning, where models predict outcomes
over action sequences and search for optimal ones [38,
39, 45].
Planning has shown great success, e.g., Al-
phaGo [46], but real-world dynamics are far harder to
predict than a Go board. Enabling real-world planning
first requires a predictive world model.
5. Conclusion
In this work, we focus on evaluating and improving a world
model’s direct predictive capability. The resulted model,
MoSim, significantly outperforms the world models in di-
rect state prediction. We show, for the first time, that when
a world model’s prediction horizon and accuracy are suffi-
cient, it is possible to train or search for a new policy purely
in the predicted space in a zero-shot manner. However, a
notable gap remains in achieving few-shot learning or even
zero-shot learning in many cases. Our findings highlight
that world models for motion dynamics is a promising di-
rection for developing more versatile and capable embodied
systems.
8
