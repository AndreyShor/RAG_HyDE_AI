[15] Drew A Hudson and Christopher D Manning. Gqa: A new
dataset for real-world visual reasoning and compositional
question answering. In Proceedings of the IEEE/CVF con-
ference on computer vision and pattern recognition, pages
6700–6709, 2019. 1
[16] Songtao Jiang, Tuo Zheng, Yan Zhang, Yeying Jin, Li Yuan,
and Zuozhu Liu.
Med-moe: Mixture of domain-specific
experts for lightweight medical vision-language models. In
Findings of the Association for Computational Linguistics:
EMNLP 2024, pages 3843–3860, 2024. 2
[17] Jihyung Kil, Zheda Mai, Justin Lee, Arpita Chowdhury, Zihe
Wang, Kerrie Cheng, Lemeng Wang, Ye Liu, and Wei-Lun
Chao. Mllm-compbench: A comparative reasoning bench-
mark for multimodal llms. In The Thirty-eight Conference on
Neural Information Processing Systems Datasets and Bench-
marks Track. 1
[18] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip:
Bootstrapping language-image pre-training for unified vision-
language understanding and generation.
In International
conference on machine learning, pages 12888–12900. PMLR,
2022. 1
[19] Lin Li, Guikun Chen, Hanrong Shi, Jun Xiao, and Long Chen.
A survey on multimodal benchmarks: In the era of large ai
models. arXiv preprint arXiv:2409.18142, 2024. 1
[20] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.
Visual instruction tuning. Advances in neural information
processing systems, 36, 2024. 1, 2
[21] Jun Liu, Zile Liu, Cong Wang, Yanhua Xu, Jiayu Chen, and
Yichun Cheng. K-12 students’ higher-order thinking skills:
Conceptualization, components, and evaluation indicators.
Thinking Skills and Creativity, 52:101551, 2024. 1
[22] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang
Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He,
Ziwei Liu, et al. Mmbench: Is your multi-modal model an
all-around player?
In European conference on computer
vision, pages 216–233. Springer, 2024. 1, 2, 3
[23] David F Lohman and Joni M Lakin. Intelligence and reason-
ing. The Cambridge handbook of intelligence, pages 419–441,
2011. 1
[24] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li,
Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel
Galley, and Jianfeng Gao. Mathvista: Evaluating mathemat-
ical reasoning of foundation models in visual contexts. In
The Twelfth International Conference on Learning Represen-
tations, 2023. 1, 2
[25] Fanqing Meng, Chuanhao Li, Jin Wang, Quanfeng Lu, Hao
Tian, Tianshuo Yang, Jiaqi Liao, Xizhou Zhu, Jifeng Dai, Yu
Qiao, et al. Mmiu: Multimodal multi-image understanding for
evaluating large vision-language models. In The Thirteenth
International Conference on Learning Representations. 2
[26] Michael Moor, Qian Huang, Shirley Wu, Michihiro Yasunaga,
Yash Dalmia, Jure Leskovec, Cyril Zakka, Eduardo Pontes
Reis, and Pranav Rajpurkar. Med-flamingo: a multimodal
medical few-shot learner. In Machine Learning for Health
(ML4H), pages 353–367. PMLR, 2023. 2
[27] Meredith Ringel Morris, Jascha Sohl-Dickstein, Noah Fiedel,
Tris Warkentin, Allan Dafoe, Aleksandra Faust, Clement Fara-
bet, and Shane Legg. Levels of agi: Operationalizing progress
on the path to agi. arXiv preprint arXiv:2311.02462, 2023. 1
[28] OpenAI. Gpt-4v(ision) system card. 2023. 2
[29] OpenAI. Gpt-4o: A multimodal language model, 2024. Ac-
cessed: 2025-03-08. 7
[30] OpenAI. Gpt-o1-mini: A multimodal language model, 2024.
Accessed: 2025-03-08. 7
[31] OpenAI. Gpt-o3-mini: A cost-effective reasoning model,
2025. Accessed: 2025-03-08. 7
[32] Shuai Peng, Di Fu, Liangcai Gao, Xiuqin Zhong, Hongguang
Fu, and Zhi Tang. Multimath: Bridging visual and mathe-
matical reasoning for large language models. arXiv preprint
arXiv:2409.00147, 2024. 2
[33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning, pages
8748–8763. PmLR, 2021. 1
[34] Navid Rajabi and Jana Kosecka. Gsr-bench: A benchmark
for grounded spatial reasoning evaluation via multimodal
llms. In NeurIPS 2024 Workshop on Compositional Learning:
Perspectives, Methods, and Paths Forward. 1
[35] Tanik Saikh, Tirthankar Ghosal, Amish Mittal, Asif Ekbal,
and Pushpak Bhattacharyya. Scienceqa: A novel resource
for question answering on scholarly articles. International
Journal on Digital Libraries, 23(3):289–301, 2022. 2
[36] Wenhao Shi, Zhiqiang Hu, Yi Bin, Junhua Liu, Yang Yang,
See Kiong Ng, Lidong Bing, and Roy Lee. Math-llava: Boot-
strapping mathematical reasoning for multimodal large lan-
guage models. In Findings of the Association for Computa-
tional Linguistics: EMNLP 2024, pages 4663–4680, 2024.
2
[37] Robert J Sternberg. Reasoning, problem solving, and intel-
ligence. Handbook of human intelligence, pages 225–307,
1982. 1
[38] Yuxuan Sun, Hao Wu, Chenglu Zhu, Sunyi Zheng, Qizi
Chen, Kai Zhang, Yunlong Zhang, Dan Wan, Xiaoxiao Lan,
Mengyue Zheng, et al. Pathmmu: A massive multimodal
expert-level benchmark for understanding and reasoning in
pathology. In European Conference on Computer Vision,
pages 56–73. Springer, 2024. 1
[39] Ryota Tanaka, Kyosuke Nishida, Kosuke Nishida, Taku
Hasegawa, Itsumi Saito, and Kuniko Saito. Slidevqa: A
dataset for document visual question answering on multiple
images. In Proceedings of the AAAI Conference on Artificial
Intelligence, pages 13636–13645, 2023. 1
[40] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-
Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk,
Andrew M Dai, Anja Hauth, Katie Millican, et al. Gemini: a
family of highly capable multimodal models. arXiv preprint
arXiv:2312.11805, 2023. 2, 7
[41] Qwen Team. Qvq: To see the world with wisdom, 2024. 7
[42] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Mar-
tinet, Marie-Anne Lachaux, Timoth´ee Lacroix, Baptiste
Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.
LLaMA: Open and efficient foundation language models.
arXiv preprint arXiv:2302.13971, 2023. 2
