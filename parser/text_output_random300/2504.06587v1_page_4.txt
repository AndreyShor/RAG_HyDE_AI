MobiSys â€™25, June 23â€“27, 2025, Anaheim, California, US
Jinbo Peng, Junwen Duan, Zheng Lin, Haoxuan Yuan, Yue Gao, and Zhe Chen
Spectrum 
sensor
Signal 
classifier
Header 
decoder
Multi-coset
sampling
Signal 
recovery
â€¦
â€¦
0110â€¦
1011â€¦
Input:
Output:
Figure 4: Signal processing pipeline of SigChord
3
System Design
SigChord consists of a low-cost sub-Nyquist sampling frontend and
a signal analysis backend. Firstly, SigChord utilizes multi-coset sub-
Nyquist sampling to alleviate the burden on the sniffing frontend.
Secondly, the backend must handle complex sequential analysis
tasks from limited sub-Nyquist samples. That is, we need to effec-
tively predict the spectrum occupancy in order to enable smoother
signal recovery path through non-blind recovery. And we need
to effectively capture signal features from time-series data in or-
der to classify, compensate, demodulate and finally decode signals.
To this end, SigChord employs Transformer-based modules for
their proven strengths in semantic analysis [67â€“69]. Besides, the
well-established and optimized tool-chains for Transformers could
enhance the availability and deployment efficiency of SigChord.
The backend of SigChord comprises three submodules: spectrum
sensing, protocol identification, and header decoding. The spectrum
sensing module identifies occupied sub-bands from sub-Nyquist
IQ-sampled data. With this information, SigChord recovers signals
from limited sub-Nyquist samples and provides downstream models
with separated signals. The protocol identification module classi-
fies frames with intact headers, filtering out those without, while
the header decoding module calibrates, demodulates, and decodes
physical layer packet headers. The overall pipeline is illustrated
in Figure 4. With a modular and software-based design, the back-
end could run flexibly on PCs, servers, or even the cloud, reducing
SigChordâ€™s reliance on resource-limited devices. Additionally, the
modular architecture ensures flexible integration and extension for
new protocols.
3.1
Sub-Nyquist Sampling and Preprocess
The sub-Nyquist sampling frontend of SigChord utilizes multi-coset
sampling, which employs a multiplexer and multiple low-speed
ADCs with unique time delays to sample the signal in parallel. Let
the multiband signal be ğ‘¥(ğ‘¡) = P
ğ‘—ğ‘¥ğ‘—(ğ‘¡), where ğ‘¥(ğ‘¡) is band-limited
within [âˆ’ğµ/2, ğµ/2]. The Nyquist rate for sampling ğ‘¥(ğ‘¡) is ğµ. The
low-speed ADCs operate at ğµ/ğ¿. multi-coset sampling employs
ğ‘ƒADCs (ğ‘ƒâ‰ªğ¿), so the total sampling rate becomes ğ‘ƒ/ğ¿of the
Nyquist rate. The samples are captured as ğ‘¦âˆˆCğ‘ƒÃ—ğ‘, with the ğ‘—-th
ADCâ€™s samples given by
ğ‘¦ğ‘—,ğ‘›= ğ‘¥
ğ‘›ğ¿+ ğ‘ğ‘—
ğµ

,
ğ‘›= 1, 2, . . . , ğ‘
(3)
where ğ‘ğ‘—is the unique offset for the ğ‘—-th ADC, and ğ‘is the number
of samples per ADC. Applying Fourier transform to Eq. (3), we get
the compressed sensing model ğ‘Œ= ğ´ğ‘‹, where ğ‘ŒâˆˆCğ‘ƒÃ—ğ‘is the
transformed samples, ğ´âˆˆCğ‘ƒÃ—ğ¿is the measurement matrix formed
by Fourier bases determined by the offsets ğ‘ğ‘—, and ğ‘‹âˆˆCğ¿Ã—ğ‘is the
spectrum matrix to be recovered. Detailed discussions can be found
in [5, 70], and we omit them here for brevity.
Instead of relying on CS methods, we incorporate a data-driven
Transformer with rule-based signal recovery. The Transformer
layer processes a sequence of high-dimensional token vectors using
an attention mechanism, where each token is correlated with oth-
ers, producing a new token sequence. The standard Transformer
architecture includes encoders and decoders: the encoder applies
self-attention to the input sequence, producing an output sequence
of the same length, while the decoder uses attention to correlate
the existing features (e.g., encoder features) with new query to-
kens, generating an output sequence matching the query length.
Each Transformer layer distributes the attention mechanism across
multiple heads, allowing attention from diverse perspectives.
To process IQ samples with a Transformer, previous methods in-
clude reshaping [66, 71], linear embeddings [71], and convolutional
neural networks (CNN) [72] for Nyquist sampled data. Given the
severe aliasing in sub-Nyquist samples, we adopt reshaping and a
3-layer MLP for nonlinear embedding, in seek of both effectiveness
and efficiency, as shown in Figure 5. Given the multi-coset samples
ğ‘¦âˆˆCğ‘ƒÃ—ğ‘, we transpose and unfold ğ‘¦into ğ‘§âˆˆRğ‘Ã—2ğ‘ƒsuch that
ğ‘§ğ‘—,2ğ‘˜+1 +ğ‘–ğ‘§ğ‘—,2ğ‘˜+2 = ğ‘¦ğ‘˜,ğ‘—, where ğ‘–is the imaginary unit. Longer input
sequences to Transformer significantly increase memory and pro-
cessing demands, therefore, we further reshape ğ‘§into ğ‘§â€² âˆˆR
ğ‘
ğ¹Ã—2ğ‘ƒğ¹,
where ğ¹is the folding factor to group IQ samples into patches and
reduce the input sequence length.
We then feed ğ‘§â€² into the MLP. The MLP consists of 3 linear layers
with Gaussian Error Linear Units (GELU) activation and dropout
after each of the first 2 layers. Chosen for its smooth non-linearity,
GELU improves gradient flow and enhances model expressiveness
compared to ReLU. The input feature size is 2ğ‘ƒğ¹, and the output
sizes are 2ğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™, 2ğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™, and ğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™, respectively. Each layer
includes layer normalization. Positional embeddings are added to
the output. Each subsequent module in SigChord is equipped with
its own reshaping and a 3-layer MLP for IQ sample embedding.
3.2
Spectrum Sensing and Signal Recovery
As discussed, accurate spectrum sensing below twice the Landau
rate allows breaking the sampling limit in [5]. We uniformly di-
vide the spectrum into ğ¿sub-bands, corresponding to the rows of
the spectrum matrix ğ‘‹in Eq. (1). SigChord formulates spectrum
ğ‘
ğ‘ƒ
ğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™
ğ‘
ğ¹
â€¦
Transpose & unfold
3-layer MLP
Sub-Nyquist 
embedding
ğ‘
ğ¹
2ğ‘ƒğ¹
â€¦
â€¦
ğ¹
2ğ‘ƒ
ğ¹
Reshape
Figure 5: The embedding process of SigChord.
