 
5 
improves its ability to produce statistically consistent samples, while the discriminator refines its ability to 
detect discrepancies. 
In the dynamics matching framework, we still employ a neural network as the classifier to detect 
discrepancies. The generator, however, is a CG model that can generate sample trajectories by propagating 
the state of the system forward in time according to the predefined dynamic equations. In the numerical 
experiments, we consider two types of dynamics: Brownian dynamics and Langevin dynamics. 
In the case of Brownian dynamics, learnable parameters ğœ½= {ğœ½', Î±, ğ›¾} include potential network 
parameters ğœƒ: for potential ğ‘ˆ(ğ‘¹, ğœ½:), a time scaling factor ğ›¼, and the diffusion coefficient ğ›¾, such that 
 
ğ‘‘ğ‘¹
ğ‘‘Ï„ = âˆ’1
Î³ âˆ‡ğ‘ˆ(ğ‘¹) + M2ğ‘˜;ğ‘‡
Î³
ğƒ(ğœ) 
Ï„ =  Î±ğ‘¡ 
 
(4) 
Here for simplicity, we assume a constant diffusion coefficient. Similarly for Langevin dynamics, with the 
extra learning parameter ğ‘š (note it need not be the real mass): 
 
ğ‘šğ‘‘<ğ‘…
ğ‘‘Ï„< = âˆ’âˆ‡ğ‘ˆ(ğ‘¹) âˆ’ğ‘‘ğ‘¹
ğ›¾ğ‘‘ğœ+ T2ğ‘˜;ğ‘‡ğ›¾ğƒ(ğœ) 
Ï„ =  Î±ğ‘¡ 
 
(5) 
Following the adversarial training framework, we optimize the learnable parameters in the dynamic 
equation by formulating the problem as a minimax optimization task: 
 
ğœƒ4 = argmax
5
Umin
= XâŒ©ğ‘“=(ğ‘¹+:-)âŒª>" âˆ’âŒ©ğ‘“=(â„³(ğ’“ğŸ:ğ‘»))âŒª?#$%\], 
 (6) 
 
where ğ‘“= represents the neural network and the expectations âŒ©âˆ™âŒª are taken separately over the CG trajectory 
ensemble ğ‘„5 and the AA trajectory ensemble ğ‘ƒ789. As illustrated in Fig.1, trajectories are sampled from 
both the AA model and CG model. The classifier is trained to give a high classification score for the 
projected trajectories from the AA model and a low classification score for the CG trajectories. 
Concurrently, the CG model is optimized according to the feedback of the classifier network, maximizing 
the classification score of the CG trajectories. To enable this, we leverage JAX-MD28 as our differentiable 
molecular dynamics engine, with the classifier network implemented directly in JAX.29 This framework 
ensures seamless integration of gradient-based optimization with the dynamics simulations. Other 
differentiable MD packages, such as TorchMD,30 offer similar capabilities. 
 
