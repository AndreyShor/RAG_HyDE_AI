References
[1]
W. Kohn and L. J. Sham. “Self-Consistent Equations Including Exchange and Correlation
Effects”. In: Phys. Rev. 140 (4A 1965), A1133–A1138. doi: 10.1103/PhysRev.140.A1133.
url: https://link.aps.org/doi/10.1103/PhysRev.140.A1133.
[2]
John P. Perdew, Kieron Burke, and Matthias Ernzerhof. “Generalized Gradient Approx-
imation Made Simple.” In: Physical review letters 77 (1996), pp. 3865–3868. url: https:
//api.semanticscholar.org/CorpusID:6425905.
[3]
Larry A. Curtiss et al. “Assessment of Gaussian-2 and density functional theories for the
computation of enthalpies of formation”. In: The Journal of Chemical Physics 106.3 (Jan.
1997), pp. 1063–1079. issn: 0021-9606. doi: 10.1063/1.473182. eprint: https://pubs.
aip.org/aip/jcp/article-pdf/106/3/1063/19100768/1063\_1\_online.pdf. url:
https://doi.org/10.1063/1.473182.
[4]
Songrit Maneewongvatana and David M. Mount. “Analysis of approximate nearest
neighbor searching with clustered point sets”. In: CoRR cs.CG/9901013 (1999). url:
https://arxiv.org/abs/cs/9901013.
[5]
Anubhav Jain et al. “Commentary: The Materials Project: A materials genome approach
to accelerating materials innovation”. In: APL materials 1.1 (2013).
[6]
Ask Hjorth Larsen et al. “The atomic simulation environment—a Python library for
working with atoms”. In: Journal of Physics: Condensed Matter 29.27 (2017), p. 273002. doi:
10.1088/1361-648X/aa680e. url: https://dx.doi.org/10.1088/1361-648X/aa680e.
[7]
A Paszke. “Pytorch: An imperative style, high-performance deep learning library”. In:
arXiv preprint arXiv:1912.01703 (2019).
[8]
Sebastian Raschka, Joshua Patterson, and Corey Nolet. “Machine Learning in Python:
Main developments and technology trends in data science, machine learning, and artificial
intelligence”. In: arXiv preprint arXiv:2002.04803 (2020).
[9]
Alvaro Sanchez-Gonzalez et al. “Learning to Simulate Complex Physics with Graph
Networks”. In: ArXiv abs/2002.09405 (2020). url: https://api.semanticscholar.org/
CorpusID:211252550.
[10]
John Jumper et al. “Highly accurate protein structure prediction with AlphaFold”. In:
Nature 596.7873 (2021), pp. 583–589. doi: 10.1038/s41586-021-03819-2.
[11]
Ilyes Batatia et al. MACE: Higher Order Equivariant Message Passing Neural Networks for Fast
and Accurate Force Fields. 2023. arXiv: 2206.07697 [stat.ML]. url: https://arxiv.org/
abs/2206.07697.
[12]
Bowen Deng et al. CHGNet: Pretrained universal neural network potential for charge-informed
atomistic modeling. 2023. arXiv: 2302.14231 [cond-mat.mtrl-sci]. url: https://arxiv.
org/abs/2302.14231.
[13]
Marcel F Langer, J Thorben Frank, and Florian Knoop. “Stress and heat flux via automatic
differentiation”. In: The Journal of Chemical Physics 159.17 (2023).
[14]
Yi-Lun Liao et al. “Equiformerv2: Improved equivariant transformer for scaling to higher-
degree representations”. In: arXiv preprint arXiv:2306.12059 (2023).
[15]
Maciej Majewski et al. “Machine learning coarse-grained potentials of protein ther-
modynamics”. In: Nature Communications 14.1 (2023), p. 5739. issn: 2041-1723. doi:
10.1038/s41467-023-41343-1. url: https://doi.org/10.1038/s41467-023-41343-1.
[16]
Jonathan Schmidt et al. “Machine-Learning-Assisted Determination of the Global Zero-
Temperature Phase Diagram of Materials”. In: Advanced Materials 35.22 (2023), p. 2210788.
doi: https://doi.org/10.1002/adma.202210788. eprint: https://onlinelibrary.
wiley.com/doi/pdf/10.1002/adma.202210788. url: https://onlinelibrary.wiley.
com/doi/abs/10.1002/adma.202210788.
11
