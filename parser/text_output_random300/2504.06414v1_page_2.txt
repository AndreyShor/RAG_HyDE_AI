manufacturing foundries, making their states accessible with higher speed. The generated bitstreams can be tuned using 
techniques such as applying spin transfer torque (STT) [22] or  spin–orbit torque (SOT) [23]. SC-based ANN using 
MTJ-generated bitstreams is reported in [24] for hand-written digit classification tasks. The stochastic bitstreams of 
continuous probabilities are generated using an analog approach [24]. 
However, to achieve ANN parameters (i.e. weights and biases) with analog precision (i.e. 32-bit) using non-volatile 
and other technologies often requires a lot of circuit overhead [25]-[27] and is more prone to device-to-device variations 
which can negatively impact the accuracy and scalability of the ANN [25]-[27]. Interestingly, ANN with extremely low 
precision or quantized weights and biases (can be even binary) are shown to achieve competitive accuracies [28]-[31] 
compared to 32-bit precision ANN. The accuracy degradation due to the quantization loss in such ANN is prevented 
during the learning stage by preserving the weight gradients [28]. The idea of quantization can be extended to SC where 
bitstreams with only a few probabilities are sufficient (i.e. 5 or 11-states) to retain the ANN accuracy. This way the 
peripheral circuit overhead and the complexity of the stochastic bitstream generating devices can be significantly 
reduced. Quantization-aware training using hardware-accelerators such as non-volatile computational devices arranged 
in crossbar architecture are reported with competitive accuracies [29]- [33], where matrix vector multiplications are 
performed by Kirchhoff’s and Ohm’s laws. In this paper, we explore the quantization aware training of SC-ANN where 
the inputs, activations and weights are represented by bitstreams of only a few quantized probabilities. The bitstream 
values with different probabilities are obtained using experiments where we bias the MTJs with different STT current 
densities [24]. During the quantization aware training, multiplication operations are simulated using bitwise XNOR 
operation (bipolar weights) and the addition operation with parallel counters. Furthermore, the sigmoid activation 
functions are implemented by using a much simpler look up table (LUT) based design due to the fact that the neuron 
output in quantized training can only assume a few discrete values (5 or 11 states). We tested the performance of our 
proposed training approach of the SC-based ANN on handwritten digit classification tasks using MNIST datasets where 
we evaluate the inference performance of such quantized SC-ANN on different topology ANNs (with 1 and 3 hidden 
layers) and explored the impact of smaller length bitstreams (such as 100-bits) on the accuracy.  Using our approach 
with smaller size bitstreams (i.e. 100 bits length), we show that we can achieve competitive accuracy on MNIST 
handwritten digit classification tasks compared to 32-bit precision similar architecture ANN with much less energy 
consumption and latency. In addition, our approach requires low precision (i.e. 3-bit) digital to analog converter (DAC) 
which can potentially reduce prohibitive energy cost.  
The rest of the paper is organized as follows. Section II provides a brief overview of stochastic computing 
implemented with random bit streams generated by s-MTJs, section III discusses the structure of deep neural networks 
and quantization, section IV presents a performance analysis and section V presents the conclusion.  
 
 
 
II. 
STOCHASTIC COMPUTING 
A. Probabilistic Mathematics  
       In stochastic computing (SC), a number is represented by the number of "1"s in its bitstream. Specifically, in 
unipolar encoding, the value of a number corresponds to the probability "1"s in its bitstream [34]. If a number X has 
N number of 1s and bitstream length is L, then stochastic representation of X will be P(X)= N/L. Multiplication of 
two numbers, X=A×B can be performed using simple AND gates, as in stochastic domain since P(X)=P(A). P(B). 
Thus, the number of transistors is significantly reduced. Unipolar encoding can only represent positive numbers in the 
range [0, 1]. However, bipolar encoding is required to represent both positive and negative numbers in the range [-1, 
1] [34]. In this scheme, a number is represented by X= 2× P(X) – 1 or alternatively, P(X)= (X + 1)/ 2. 
To accommodate negative weights in the neural network, bipolar encoding has been used. XNOR gate can be used 
for multiplication of two bipolar numbers since P(X)= P(A).P(B) + P(A) . P(B) and for bipolar encoding it becomes 
(X+1)/2 = [(A+1)/2] × [(B+1)/2] + [1- (A+1)/2] × [1- (B+1)/2] which produces X=A.B [34]. Thus, the number of 
transistor gates required for arithmetic operations is also reduced significantly in this case. 
 
 
B. Bitstream Generation:  
Using CMOS devices as shown in Figure 1(a), a random number generator can convert binary numbers to stochastic 
bitstreams. An alternative and more energy efficient way of generating Random Numbers is by using a MTJ shown in 
