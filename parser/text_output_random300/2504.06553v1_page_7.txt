Method
s-acc (%)
t-acc (%)
Hydra [14] + GPT
8.18
2.44
HOV-SG [45]
8.98
1.95
ASHiTA
21.7
8.78
Hydra (GT Seg) + GPT
14.2
6.34
Table 2.
Evaluation of sequential grounding on 8 scenes from the SG3D
HM3DSem dataset with incrementally built scene-graph representations.
Trials with knowledge of the ground truth object labels are highlighted.
with Chat-GPT-4o-mini to form the initial task hier-
archy. We only run half of the Hierarchy Refinement de-
scribed in Sec. 4.3 to only update the items for each subtask
and omit the step to add additional new subtasks.
We first evaluate the performance when given the ground
truth 3D instance segmentation. We compare the perfor-
mance of ASHiTA with image embeddings and ASHiTA
with the text embeddings of the ground-truth labels against
state-of-the-art zero-shot 3D visual grounding models (3D-
VisTA [52] and PQ3D [53]) as benchmarked in [50]. We
use two metrics in our evaluation [50]: subtask-accuracy (s-
acc) and task-accuracy (t-acc) for given regions of the scene.
The subtask-accuracy is the percentage of subtasks that are
grounded to the correct object, and the task-accuracy is
the percentage of tasks with subtasks that are all correctly
grounded. The results are shown in Table 1. ASHiTA’s
performance is significantly better than the baseline zero-
shot models. The variants with ground truth information
demonstrates performance without segmentation noise and
with ideal visual-language alignment. In this limit, ASHiTA
surpasses the best fine-tuned baseline in SG3D [50] and ap-
proaches the performance of GPT with ground-truth labels
– falling short due to the lack of explicit relational informa-
tion (e.g. “next to”, “on top of”) in ASHiTA.
To evaluate approaches better suited for embodied appli-
cations that incrementally build scene graphs, we also use 8
RGB-D sequences generated by [45] from 8 HM3DSem test
scenes. This corresponds to 205 high-level tasks from the
SG3D dataset [50]. Our baselines for this experiment are
to use GPT to ground the task hierarchy on Hydra [15] and
HOV-SG [45] generated scene graphs. For Hydra, we eval-
uate with both ground-truth 2D instance segmentation and
with a closed-set segmentation model (OneFormer [16]),
crop the resulting scene graph to the given evaluation re-
gion, then convert the resulting scene graph to a query-able
format: {Room1: {Object 1: {label: <object-name>, posi-
tions: (x, y, z)}, Object2: ...}, Rom2: {}}. This is given to
GPT-4o-mini alongside the tasks and the subtasks, and
GPT identifies the node associated to each subtask. For
HOV-SG, we also first crop the scene graph of the full scene
to the given region, then we query the scene graph with the
subtasks as described in [45] using GPT-4o-mini to re-
trieve the grounded object for each subtask. For evaluation,
we require the centroid of the estimated object bounding
Method
s-rec (%) s-prec (%) t-acc (%)
Hydra + GPT
9.43
15.51
4.88
HOV-SG + GPT
4.55
4.87
1.95
ASHiTA
10.39
20.6
9.27
ASHiTA (GT Pos)
15.12
34.47
16.59
Hydra (GT Seg) [14] + GPT
17.06
18.98
14.63
ASHiTA (GT Pos + Txt Emb)
38.71
34.39
36.1
Table 3.
Evaluation of Hierarchical Task Analysis. Gray highlight indi-
cates directly using ground-truth object positions. Blue highlight marks
trials having knowledge of ground-truth labels.
box to be contained by the ground-truth object bounding
box to be considered a correct match. The results are shown
in Table 2. We show that ASHiTA significantly outperforms
the GPT-powered baselines, even when ground-truth seg-
mentation is used in Hydra, cf. Hydra (GT Seg) in the table.
5.2. Hierarchical Task Analysis
We adapted the SG3D dataset [50] to evaluate hierarchical
task analysis using the given subtasks as the reference task
hierarchy for each high-level task. To evaluate a generated
task hierarchy, we count an estimated subtask as correct if it
grounds to the same set of objects as a reference subtask and
incorrect otherwise. A reference subtask is missed if there
is no estimated subtask that grounds to its set of objects. We
report three metrics: subtask recall, step precision, and task
accuracy. Given C correct subtasks, I incorrect subtasks,
and M missed reference subtasks, the subtask recall (s-rec)
is defined as
C
C+M , and the subtask precision (s-prec) is
defined as
C
C+I . The task accuracy (t-acc) is the percentage
of tasks with no incorrect subtasks.
We utilize the 8 RGB-D sequences [45] and the asso-
ciated 205 high-level tasks for evaluation. We again use
GPT-4o-mini and the scene graphs generated by Hy-
dra [14] and HOV-SG [45] as baselines for evaluation. In-
stead of asking ChatGPT to identify the node associated to
each subtask, we ask for the subtasks and nodes given the
query-able scene graph and the high-level task. The results
are shown in Table 3. ASHiTA outperforms the baselines by
a large margin. The GPT-powered baselines tend to provide
a large number of subtasks that include many contextually
relevant but redundant and sometimes tangential subtasks,
and this can be seen in particular with the lower subtask
precision for Hydra with GPT. HOV-SG is not well suited
to parsing high-level tasks, as it expects a formatted hierar-
chical query that is aligned with the scene graph structure.
5.3. System Ablations
We perform ablations on the components of ASHiTA, com-
paring the full ASHiTA pipeline against (i) replacing the
H-IB with recursively running the original IB algorithm,
(ii) removing the Top-Down pruning step (Sec. 4.2), (iii)
removing the Spatial Update (Sec. 4.3), and (iv) remov-
7
