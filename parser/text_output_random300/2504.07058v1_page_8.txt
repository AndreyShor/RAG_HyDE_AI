2.6
Residuals
This section describes the residuals linked to different training sets, including interior, temporal, spatial and
data points used for inverse problems. The primary objective is to minimize these residuals. Optimization
will incorporate stochastic gradient descent techniques, such as ADAM for first-order optimization, along with
higher-order methods like variants of the BFGS algorithm. The PINN ğ‘¢ğœƒdepends on tuning parameters ğœƒâˆˆğœƒâ€²,
which correspond to the networkâ€™s weights and biases. In a standard deep learning framework, training involves
adjusting these parameters to ensure that the neural network approximation ğ‘¢ğœƒclosely matches the exact solution
ğ‘¢. The interior residual is defined as:
â„œint,ğœƒ= â„œint,ğœƒ(ğ‘¡, ğ‘¥),
âˆ€(ğ‘¡, ğ‘¥) âˆˆ[0,ğ‘‡] Ã— D.
(2.24)
It can be expressed in terms of the differential operator as follows:
â„œint,ğœƒ= D(ğ‘¢ğœƒ) âˆ’f.
(2.25)
The residual formulation of our models, given in Eq. (2.6) and Eq. (2.10), can be written as:
â„œint,ğœƒ= ğœ•ğ‘¡ğ‘¢ğœƒâˆ’0.5ğœ•ğ‘¥ğ‘¥ğ‘¢ğœƒâˆ’ğ‘…(ğ‘¢ğœƒ)
â„œint,ğœƒ= ğœ•ğ‘¡ğ‘¢ğœƒ+ ğ›¾âˆ†2ğ‘¢ğœƒâˆ’âˆ†ğ‘¢ğœƒ+ ğ¹(ğ‘¢ğœƒ).
(2.26)
Residuals corresponding to initial, boundary, and data points are formulated as:
â„œtb = â„œtb,ğœƒ= ğ‘¢ğœƒâˆ’ğ‘¢0,
âˆ€ğ‘¥âˆˆD,
â„œsb = â„œsb,ğœƒ= ğ‘¢ğœƒâˆ’ğ‘¢ğ‘,
âˆ€(ğ‘¡, ğ‘¥) âˆˆğœ•D.
(2.27)
Additionally, the residual for data points is given by:
â„œğ’…= L(ğ‘¢ğœƒ) âˆ’ğ‘”,
âˆ€(ğ‘¡, ğ‘¥) âˆˆDâ€²
ğ‘‡.
(2.28)
The goal is to determine the optimal tuning parameters ğœƒâˆˆğœƒâ€² that minimize the residual in the forward problem:
ğœƒâˆ—âˆˆğœƒâ€² : ğœƒâˆ—= arg min
ğœƒâˆˆğœƒâ€²

âˆ¥â„œint,ğœƒâˆ¥2
ğ¿2(Dğ‘‡)+âˆ¥â„œsb,ğœƒâˆ¥2
ğ¿2([0,T]Ã—ğœ•D)+âˆ¥â„œtb,ğœƒâˆ¥2
ğ¿2(D)

.
(2.29)
For the inverse problem, an additional term corresponding to the data residual Rğ’…is introduced in Eq. (2.29),
leading to the following minimization problem:
ğœƒâˆ—âˆˆğœƒâ€² : ğœƒâˆ—= arg min
ğœƒâˆˆğœƒâ€²

âˆ¥â„œint,ğœƒâˆ¥2
ğ¿2(Dğ‘‡)+âˆ¥â„œsb,ğœƒâˆ¥2
ğ¿2([0,T]Ã—ğœ•D)+âˆ¥â„œğ’…,ğœƒâˆ¥2
ğ¿2(Dâ€²
ğ‘‡)+âˆ¥â„œtb,ğœƒâˆ¥2
ğ¿2(D)

.
(2.30)
Since the integrals in Eqs. (2.29) and (2.30) involve the ğ¿2 norm, an exact computation is not feasible. Instead,
numerical quadrature methods are employed for approximation.
2.7
Loss Functions and Optimization
The integrals (2.25) is approximated using the following loss functions for forward problems:
â„’1(ğœƒ) =
ğ‘ğ‘ ğ‘
âˆ‘ï¸
ğ‘—=1
ğ‘¤ğ‘ ğ‘
ğ‘—|â„œsb,ğœƒ(ğ‘§ğ‘ ğ‘
ğ‘—)|2+
ğ‘ğ‘¡ğ‘
âˆ‘ï¸
ğ‘—=1
ğ‘¤ğ‘¡ğ‘
ğ‘—|â„œtb,ğœƒ(ğ‘§ğ‘¡ğ‘
ğ‘—)|2+ğœ†
ğ‘ğ‘–ğ‘›ğ‘¡
âˆ‘ï¸
ğ‘—=1
ğ‘¤ğ‘–ğ‘›ğ‘¡
ğ‘—
|â„œint,ğœƒ(ğ‘§ğ‘–ğ‘›ğ‘¡
ğ‘—)|2,
(2.31)
The integrals (2.27) is approximated using the following loss functions for inverse problems:
â„’2(ğœƒ) =
ğ‘ğ‘‘
âˆ‘ï¸
ğ‘—=1
ğ‘¤ğ‘‘
ğ‘—|â„œğ’…,ğœƒ(ğ‘§ğ‘‘
ğ‘—)|2+
ğ‘ğ‘ ğ‘
âˆ‘ï¸
ğ‘—=1
ğ‘¤ğ‘ ğ‘
ğ‘—|â„œsb,ğœƒ(ğ‘§ğ‘ ğ‘
ğ‘—)|2+
ğ‘ğ‘¡ğ‘
âˆ‘ï¸
ğ‘—=1
ğ‘¤ğ‘¡ğ‘
ğ‘—|â„œtb,ğœƒ(ğ‘§ğ‘¡ğ‘
ğ‘—)|2+ğœ†
ğ‘ğ‘–ğ‘›ğ‘¡
âˆ‘ï¸
ğ‘—=1
ğ‘¤ğ‘–ğ‘›ğ‘¡
ğ‘—
|â„œint,ğœƒ(ğ‘§ğ‘–ğ‘›ğ‘¡
ğ‘—)|2.
(2.32)
The loss function minimization is regularized as follows:
ğœƒâˆ—= arg min
ğœƒâˆˆğœƒâ€²(â„’ğ‘–(ğœƒ) + ğœ†ğ‘Ÿğ‘’ğ‘”â„’ğ‘Ÿğ‘’ğ‘”(ğœƒ)),
(2.33)
where ğ‘–= 1, 2. In deep learning, regularization helps prevent over-fitting. A common form of regularization is
â„’reg(ğœƒ) = âˆ¥ğœƒâˆ¥ğ‘
ğ‘, where ğ‘= 1 (for ğ¿1 regularization) or ğ‘= 2 (for ğ¿2 regularization). The regularization parameter
ğœ†reg balances the trade-off between the loss function â„’and the regularization term, where 0 â©½ğœ†reg â‰ª1. Stochastic
gradient descent algorithms such as ADAM will be used for optimization, as they are widely adopted for first-order
8
