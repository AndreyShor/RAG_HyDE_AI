6
2.0
1.5
1.0
log10 (k [km
1 s])
0.2
0.4
0.6
0.8
1.0
1.2
1.4
PF / Pref
fWDM = 1
mWDM = 4.0 keV
mWDM = 3.0 keV
mWDM = 2.0 keV
mWDM = 1.0 keV
2.0
1.5
1.0
log10 (k [km
1 s])
0.2
0.4
0.6
0.8
1.0
1.2
1.4
mWDM = 2.0 keV
fWDM = 0.125
fWDM = 0.25
fWDM = 0.5
fWDM = 1.0
Â±  Boera+19
FIG. 3: Flux power spectrum ratio of simulated models to the reference model for CDM cosmology with post-processed parameters fixed to
fiducial values (ğ‘‡0 = 10700 K, ğ›¾= 1.2) at ğ‘§= 4.2. Left: The WDM particle mass varied with fixed ğ‘“WDM = 1. Right: CWDM models with
vaying ğ‘“WDM for ğ‘šWDM = 2 keV.
D.
Patchy correction
Cosmic reionization is not homogeneous but occurs in three
stages: the formation of H II bubbles, their overlap due to
increased number of ionizing sources, and the disappearance
of the last remaining neutral hydrogen regions as a uniform
UV background forms. As pointed out in previous work that
has used the same suite of simulations (e.g. [46, 74]), the non-
uniform nature of the reionization process leaves an imprint on
the Lyman-ğ›¼forest at both large and small scales. The former
consists of an enhancement of power at scales ğ‘˜< 0.03 kmâˆ’1s
more noticeable at higher redshifts as well as thermal histories
with an earlier end of reionization. This can be understood
through the temperature dependence of the recombination co-
efficient already mentioned in Subsection III B, which means
that fluctuations in the gas temperature contribute to the varia-
tions of the neutral hydrogen fraction. At small scales, patchy
reionization causes suppression in the power spectrum mainly
through thermal broadening and differences in peculiar veloc-
ity for regions that reionized recently [74, 107]. We account
for this extra power by computing the correction tabulated in
[74] following [46].
IV.
STATISTICAL MEASURES OF THE FLUX
DISTRIBUTION
The flux power spectrum models are defined on a six-
dimensional grid per redshift bin, for each combination of
model parameters (ğ‘‡0ğ‘§ğ‘–, ğ›¾ğ‘§ğ‘–, ğœğ‘§ğ‘–
eff, ğ‘¢0ğ‘§ğ‘–, ğ‘šWDMâˆ’1, ğ‘“WDM) =
âˆ’â†’ğœƒ. Comparing power spectra derived from observations and
simulations enables us to constrain the free parameters of the
model, âˆ’â†’ğœƒ, a process typically conducted within a Bayesian
Monte Carlo Markov Chain (MCMC) framework.
In this
study, we utilize the Cobaya code [108] to sample the Gaus-
sian multivariate likelihood function from [69] at each redshift.
However, our approach differs from previous works in the in-
terpolation scheme used to generate simulated power spectra
at each grid point. Specifically, we employ a machine learning
model capable of predicting the power spectra within a fraction
of a second on a single CPU.
A.
Neural network emulator
Given that âˆ’â†’ğœƒâˆˆR6, evaluating the likelihood at each grid
point is limited by the computation cost of interpolating in
such a high-dimensional space. We circumvent this problem
by employing a neural network. This neural network maps the
input parameters ğœƒto the output parameters ğ‘ƒF(ğ‘˜ğ‘š), in the
form ğ‘ƒF(ğ‘˜ğ‘š) = ğ‘“[ âˆ’â†’ğœƒ, ğœ™], where ğœ™are the hyperparameters
that describe the relationship between input and output, and
ğ‘˜ğ‘šis a finite set of wavenumbers matching the ones reported
in the measurements of [69].
As an initial framework, we adopt the model architecture
proposed by [82] and make two key observations. First, the
original work assessed the emulation error using ğ‘˜-fold cross-
validation on the training set itself. This approach likely leads
to overfitting, reducing the modelâ€™s ability to generalize to
unseen data, which is precisely the role of an independent test
set. Secondly, we can enhance the performance of the emulator
by using a combination of baseline models, also known as
ensemble learning. In particular, we consider averaging the
output from a range of models trained using ğ‘˜-fold cross-
validation.
We begin by randomly splitting the dataset into a 90% train-
ing set and a 10% test set, with the validation set included
within the training subset.
We then divide the training set
into ğ‘˜subsets, and perform the ğ‘˜-fold cross-validation. In this
way, each neural network is trained on ğ‘˜-1 folds of the training
data, with the remaining one being used as validation. We use
ğ‘˜= 10, therefore repeating the process iteratively 10 times.
This results in 10 neural network emulators, each validated on
