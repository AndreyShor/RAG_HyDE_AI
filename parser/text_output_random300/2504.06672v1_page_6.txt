Method
FVD (↓)
DINO-S (↓)
Latency (s) (↓)
Retrieved Videos
117.22
1.00
-
ZeroScope
613.15
0.74
17.78
FreeInit
453.50
0.79
68.88
RAGInit
422.10
0.82
19.86
RAGME
270.26
0.84
22.43
Table 1. Comparison between the baseline methods and RAGME
on the WebVid10M validation set.
Training is performed with an effective batch size of 16,
distributed on 4 Nvidia A100 GPUs.
4. Experiments
In this section, we qualitatively and quantitatively analyze
the performance of RAGME. We start by evaluating estab-
lished metrics in the video generation field on the valida-
tion set of WebVid10M [1]. Moreover, we follow VBench
[23], a benchmark recently introduced, which exploits an
array of pre-trained models to evaluate the generated videos
under multiple angles. Next, we present a series of abla-
tion studies to understand the role of each component in
our pipeline. Lastly, we showcase several qualitative results
comparing our method with the baselines.
Baselines and Setting
We compare RAGME with videos
produced by the base T2V model, ZeroScope [47]. Next,
we enhance the videos generated by the base model using
FreeInit [55], a training-free technique that optimizes the
starting noise of the diffusion process through repeated de-
noising. Finally, we compare our full model with another
baseline, which uses our proposed RagInit technique to ini-
tialize the noise.
We perform inference from all the models using the
DDIM sampler [46] with 50 denoising steps, and classifier-
free guidance with scale of s = 7.5.
4.1. Quantitative Results
WebVid10M Results
Our end goal is to develop a system
with better video quality, especially in the temporal dynam-
ics, while avoiding leakage of the conditioning videos (see
Sec. 1). To capture the first aspect, we rely on the Fr´echet
Video Distance (FVD) [49], which is well-established in the
video generation literature. To estimate the second factor,
i.e. possible copy-paste artifact from the retrieved videos,
we compute the cosine similarity on the DINOv2 [37] em-
bedding space.
Specifically, given a generated video Y
and a set of retrieved videos Z, the metric is computed as
maxZ cos-sim(Y, Zi). In this case, a model that achieves a
lower cosine similarity is considered better. Lastly, we com-
pare the methods on the latency, i.e. the time to generate a
single video. We take into account the time of retrieving the
videos and encoding them with CLIP when computing the
latency of our model. We refer to the Supp. Mat. for more
detailed discussion.
We conduct the experiments on the WebVid10M valida-
tion set, which comprises 5000 videos with the associated
captions. We report the results in Tab. 1, wherein the first
row we report the results of the retrieved videos (i.e. videos
form the WebVid10M training set) as a reference. RAGME
drastically outperforms the base diffusion model in terms of
FVD, resulting in videos of higher quality. While applying
FreeInit does lead to some improvement, it remains inferior
in comparison. RagInit achieves comparable performance
to FreeInit. However, a notable difference emerges in la-
tency: our proposed noise initialization method does not
require costly denoising steps and instead uses the retrieved
samples for noise initialization.
Analyzing the DINO-similarity metric, we observe that
RAGME shows an increase compared to both the base-
line and FreeInit. However, compared to RagInit, the full
model’s improvement is minimal, suggesting that the pri-
mary issue may lie in the noise initialization procedure
rather than the cross-attention conditioning. It is important
to note that a very low DINO cosine similarity is not desir-
able as well, and would indicate: either a lack of relevance
between the retrieved videos and the final video or a failure
of the T2V model to align with the prompt.
VBench Results
While the FVD metric is well estab-
lished, it is difficult to interpret as improvements over it can
be due to multiple factors. To get a better understanding of
what aspects our method is improving, we follow VBench
[23] for a more detailed evaluation. VBench is a recently
proposed benchmark for T2V models, which comprises a
suite of roughly 900 prompts and a list of 16 dimensions for
evaluations. In the main paper, we report only the metrics
related to the temporal consistency and quality of motion,
as these represent our main target for improvement. How-
ever, we refer the reader to the Supp. Mat. for full com-
parison between the methods, and to the original paper for
detailed explanation of how each metric is computed. We
report the results in Tab. 2. Our method strongly outper-
forms the baseline in two aspects: the Human Action and
the Dynamic Degree metrics. This reflects our design goals
of having less static videos with better motion. At the same
time comes at the price of a slight decrease in background
and subject consistency, which is nevertheless expected (a
static video would achieve a perfect score in these metrics).
Comparing with the noise initialization stargeies of FreeInit
and RagInit, it is interesting to notice that a better action
can be primarily explained by a better noise initialization,
but the dynamic degree is mostly due to the corss-attention
layers which incorporates the retrieved videos.
6
