consistent with the experiment [190].
What eventually happened is that QED was shown to
be a successful (strictly renormalizable) QFT of electrons, positrons, and photons by Sin-Itiro
Tomonaga [8,9], Julian Schwinger [10–12] and Richard Feynman [13,14]. In addition to explaining
the Lamb shift, QED was able to make new predictions for the anomalous magnetic moment of
the electron [12], which was later shown to be in spectacular agreement with experiments. In the
same years, Freeman Dyson put the concept of renormalizability by power counting on a more
rigorous basis [2, 3], and soon after Susumu Kamefuchi, Shoichi Sakata and Hiroomi Umezawa
clarified the difference between renormalizable and non-renormalizable QFTs [4], inspired by
Heisenberg’s classification of the interactions [191].
“Electroweak” and “strong” challenges
The QED party did not last long, since in the 1950s new challenges arose and plagued QFT for
roughly twenty years. While renormalization theory worked very well for the electromagnetic in-
teraction, it was realized that the same was not true for Fermi’s theory of beta decay [15]. Most of
the ingredients of the electroweak theory were already available in the 1960s [16–21], but somehow
they were not considered physically relevant because non-Abelian gauge theories were believed
to be non-renormalizable. Furthermore, it seemed that the tools of perturbative QFT were not
reliable for studying the strong interaction due to the large values of the interaction couplings. At
the same time, in 1968 James Bjorken [192,193], and subsequently Richard Feynman [194,195],
noticed that scaling properties of hadrons in deep inelastic collisions could be well explained by
assuming that hadrons behave as objects made of (almost) non-interacting constituents. This
behavior, known as Bjorken scaling, seemed to disfavor a possible QFT description of hadrons.
Another puzzle was the Sutherland-Veltman paradox [196,197], according to which existing theo-
retical methods were predicting a nearly vanishing rate for the π0 →γγ decay, while experiments
were showing the opposite. On top of all these headaches, the discovery of gauge anomalies added
further concerns about the internal consistency of the entire renormalization apparatus.
Alternative approaches.
The situation was really a mess, and it is quite understand-
able that many physicists began to be seriously skeptical about the whole QFT framework.
Lev Landau noted that known perturbation theories were plagued by Landau poles and became
one of the leading figures in believing that a paradigm shift was needed [198, 199]. Alternative
approaches that did not involve fields and Lagrangians began to be proposed. The S-matrix
theory was revived by Geoffrey Chew in the early 1960s [200] to study strong interactions and
its framework was mainly based on dispersion relations [201,202]. This was followed by the dual
resonance models and the birth of string theory [203–208]. The framework of current algebra,
based on currents instead of fields and on conservation laws instead of Lagrangians, was pro-
posed by Murray Gell-Mann [209]. Furthermore, in 1969 Tsung-Dao Lee and Gian Carlo Wick
proposed a higher-derivative extension of QED in the hope of obtaining a QFT that could be
20
