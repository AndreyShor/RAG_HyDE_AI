Differentiating both sides gives:
|âˆ†ğ‘…(ğ‘ˆğ‘›)|â©½ğ¶ğ‘’âˆ’ğœ|ğ‘ˆğ‘›| |âˆ†ğ‘ˆğ‘›|.
(2.107)
Taking the ğ¿2(D)-inner product with âˆ†ğ‘ˆğ‘›, We obtain:

âˆ†ğ‘…(ğ‘ˆğ‘›), âˆ†ğ‘ˆğ‘›
â©½ğ¶
Z
D
ğ‘’âˆ’ğœ|ğ‘ˆğ‘›| |âˆ†ğ‘ˆğ‘›|2ğ‘‘ğ‘¥.
(2.108)
Since ğ‘’âˆ’ğœ|ğ‘ˆğ‘›| is strictly decreasing, there exists a constant ğ¶0 > 0 such that:
ğ‘’âˆ’ğœ|ğ‘ˆğ‘›| â©½ğ‘’âˆ’ğœâˆ¥ğ‘ˆğ‘›âˆ¥ğ¿âˆâ©½ğ‘’âˆ’ğœğ¶0.
(2.109)
Thus, we get:

âˆ†ğ‘…(ğ‘ˆğ‘›), âˆ†ğ‘ˆğ‘› â©½ğ¶1ğ‘’âˆ’ğœğ¶0 âˆ¥âˆ†ğ‘ˆğ‘›âˆ¥2
ğ¿2(D).
(2.110)
This leads to the energy inequality:
ğ‘‘
ğ‘‘ğ‘¡âˆ¥âˆ†ğ‘ˆğ‘›âˆ¥2
ğ¿2(D)â©½âˆ’ğœ†âˆ¥âˆ†ğ‘ˆğ‘›âˆ¥2
ğ¿2(D),
(2.111)
where ğœ†= ğ¶1ğ‘’âˆ’ğœğ¶0 is positive. Applying GrÂ¨onwallâ€™s inequality, we conclude:
âˆ¥âˆ†ğ‘ˆğ‘›(ğ‘¡)âˆ¥ğ¿2(D)â©½âˆ¥âˆ†ğ‘ˆğ‘›(0)âˆ¥ğ¿2(D)ğ‘’âˆ’ğœ†ğ‘¡.
(2.112)
Thus, âˆ¥âˆ†ğ‘ˆğ‘›âˆ¥ğ¿2(D) decays exponentially over time.
Taking the time derivative of both sides of the Burgess equation, multiplying by ğ‘ˆğ‘›
ğ‘¡, and applying GrÂ¨onwallâ€™s
inequality, we obtain the boundedness of ğ‘ˆğ‘›
ğ‘¡. Combining this with the Aubin-Lions compactness theorem and
the uniform boundedness of {ğ‘ˆğ‘›}, {âˆ†ğ‘ˆğ‘›}, and {ğœ•ğ‘¡ğ‘ˆğ‘›}, there exists a subsequence of {ğ‘ˆğ‘›}ğ‘›âˆˆN (still denoted by
{ğ‘ˆğ‘›}ğ‘›âˆˆN), which converges to some function
ğ‘ˆâˆˆğ¶([0,ğ‘‡]; ğ»1(D)) âˆ©ğ¿2([0,ğ‘‡]; ğ»2(D)).
(2.113)
Moreover, ğ‘ˆğ‘›converges strongly to ğ‘ˆin ğ¿2(D). By the Arzel`a-Ascoli theorem, ğ‘ˆğ‘›uniformly converges to ğ‘ˆin
D.
â–¡
To establish the convergence of the EFK equation, we follow the approach used for the Cahn-Hilliard equation
in [56].
Theorem 2.6. Under the assumptions of Lemma Appendix E.4 and nonlinear term ğ¹satisfies Appendix E.7,
there exists a unique solution ğ‘¢âˆˆğ»2(D) âˆ©ğ»4(D) to the EFK equation 2.10. Moreover, if the sequence {ğ‘ˆğ‘›} is
uniformly bounded and equicontinuous, then the neural network approximation ğ‘ˆğ‘›converges strongly to ğ‘¢in ğ¿2(D).
Furthermore, ğ‘ˆğ‘›uniformly converges to ğ‘¢in D.
Proof. The proof is provided in Appendix E.3.
â–¡
3
Numerical Experiments
The PINN algorithms (2.1) and (2.2) were implemented using the PyTorch framework [40].
All numerical
experiments were conducted on an Apple Mac-Book equipped with an M3 chip and 24 GB of RAM. Several
hyper-parameters play a crucial role in the PINN framework, including the number of hidden layers ğ¾âˆ’1, the width
of each layer, the choice of activation function ğœ, the weighting parameter ğœ†in the loss function, the regularization
coefficient ğœ†reg in the total loss and the optimization algorithm for gradient descent. The activation function ğœis
chosen as the hyperbolic tangent (tanh), which ensures smoothness properties necessary for theoretical guarantees
in neural networks. To enhance convergence, the second-order LBFGS optimizer is employed. For optimizing the
remaining hyper-parameters, an ensemble training strategy is used, following the methodology in [4,33â€“35,37]. This
approach systematically explores different configurations for the number of hidden layers, layer width, parameter ğœ†,
and regularization term ğœ†reg, as summarized in Table 1. Each hyper-parameter configuration is tested by training
the model ğ‘›ğœƒtimes in parallel with different random weight initializations. The relative ğ¿2 error and training loss
are denoted as Eğ‘Ÿ
ğºand Eğ‘‡, respectively. The configuration that achieves the lowest training loss is selected as the
optimal model. Numerical experiments have been conducted with a maximum of 5000 LBFGS iterations.
3.1
Forward Problem
The forward problems for both models are discussed as follows:
16
