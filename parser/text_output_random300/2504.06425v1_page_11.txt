NEURAL NETWORK POLYCONVEXIFICATION
11
relaxation algorithms. Throughout all of the numerical experiments, we denote the analytical
polyconvex envelope by Φpc and by Φpc
pred the neural network prediction.
5.1. The Kohn–Strang–Dolzmann Function. This function was studied in [KS86a, KS86b],
subsequently modified to achieve continuity in [Dol99, DW00] and further studied in [Bar05] and
serves as a first benchmark example to illustrate the possibility of the neural network approach
to approximate the polyconvex envelope. We consider the function W : R2×2 →R , defined as
W(F) =
(
1 + |F|2
if |F| ≥
√
2 −1,
2
√
2 |F|
else,
where |F| := (Pd
i,j=1 F 2
ij)1/2 denotes the Frobenius norm of the matrix F. The polyconvex
envelope of W is explicitly known, see e.g. [Dol99], and reads
W pc(F) =
(
1 + |F|2
if ρ(F) ≥1,
2 (ρ(F) −|det(F)|)
else,
where ρ(F) :=
p
|F|2 + 2|det(F)|. The functions W and W pc are isotropic; consequently, they
can be rewritten in terms of the signed singular values and reduce to Φ, Φpc : R2 →R with
Φ(ˆν) =
(
1 + ν2
1 + ν2
2
if
p
ν2
1 + ν2
2 ≥
√
2 −1,
2
√
2
p
ν2
1 + ν2
2
else,
and
(16)
Φpc(ˆν) =
(
1 + ν2
1 + ν2
2
if ρ(ˆν) ≥1,
2 (|ν1| + |ν2| −|ν1ν2|)
else,
where ρ(ˆν) = |ν1| + |ν2|.
5.1.1. Network Architecture and Learning Data. In this example, we implement a FICNN which
consists of two hidden layers, with 10 and 20 neurons, respectively, as presented in Figure 3.
The learning domain for ˆν is defined as the box [−ν, ν]2 with ν = 1.05. For the training, the set
of signed singular values ˆν used as input is generated by discretising each axis with 751 points,
so that the point 0 is included, making 7512 training data points in total. Instead of using
a uniform distribution of the points, we use a local refinement towards the origin, utilising a
quadratic transformation. This refinement approach increases the data density near the point
of non-differentiability of both functions Φ and Φpc. For the validation dataset, we randomly
sample 169,200 points within the domain [−ν, ν]2, accounting for 30% of the training dataset
size. The target values, i.e. the polyconvex envelope, are obtained by evaluation of the analytical
function Φpc as in (16). For the loss function, we choose the penalty parameters λineq = 1.5 and
λsym = 1.
5.1.2. Numerical Results. On average, for a single realisation, the training process requires 21 ±
4 minutes to complete 77 ± 17 epochs. The final training error is 1.47 × 10−3 ± 2.71 × 10−4, while
the validation error reaches 1.59 × 10−3 ± 3.45 × 10−4. The learning curves for both training and
validation errors on one exemplary realisation are presented in Figure 3.
Mean error
Rel. quadratic error
Rel. max error
0.027 ± 2.59 × 10−3
0.021 ± 1.80 × 10−3
0.031 ± 3.37 × 10−3
Table 1. Average of prediction errors over ten network realisations.
Table 1 presents the approximation errors of the predictions, where the analytical polyconvex
envelope is used as a ground truth. These errors are computed on a uniform 100×100 discretisation
of the domain [−ν, ν]2. The results indicate that the predicted envelopes deviate by about 2% to
3% from the analytical polyconvex envelope Φpc, demonstrating an accuracy sufficient for the
intended engineering applications. The approximation quality is further illustrated in Figure 4,
