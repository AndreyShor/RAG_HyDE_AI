[24] Nishanth Kumar, Fabio Ramos, Dieter Fox, and Cae-
lan Reed Garrett. Open-world task and motion planning via
vision-language model inferred constraints. arXiv preprint
arXiv:2411.08253, 2024. 3
[25] Mathieu Labbé and François Michaud. Rtab-map as an open-
source lidar and visual simultaneous localization and map-
ping library for large-scale and long-term online operation.
J. of Field Robotics, 36:416 – 446, 2018. 3
[26] Philipp Lindenberger, Paul-Edouard Sarlin, and Marc Polle-
feys.
Lightglue: Local feature matching at light speed.
Intl. Conf. on Computer Vision (ICCV), pages 17581–17592,
2023. 3
[27] Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi
Zhang, Joydeep Biswas, and Peter Stone. Llm+p: Empower-
ing large language models with optimal planning proficiency.
arXiv preprint: 2304.11477, 2023. 2
[28] Peiqi Liu, Zhanqiu Guo, Mohit Warke, Soumith Chintala,
Chris Paxton, Nur Muhammad Mahi Shafiullah, and Lerrel
Pinto. Dynamem: Online dynamic spatio-semantic mem-
ory for open world mobile manipulation.
arXiv preprint
arXiv:2411.04999, 2024. 2
[29] Xinyu Liu, Houwen Peng, Ningxin Zheng, Yuqing Yang,
Han Hu, and Yixuan Yuan. Efficientvit: Memory efficient vi-
sion transformer with cascaded group attention. IEEE Conf.
on Computer Vision and Pattern Recognition (CVPR), pages
14420–14430, 2023. 3
[30] D. Maggio, Y. Chang, N. Hughes, M. Trang, D. Griffith,
C. Dougherty, E. Cristofalo, L. Schmid, and L. Carlone.
Clio: Real-time task-driven open-set 3D scene graphs. IEEE
Robotics and Automation Letters (RA-L), 2024. 1, 2, 5
[31] J. McCormac, A. Handa, A. J. Davison, and S. Leutenegger.
SemanticFusion: Dense 3D Semantic Mapping with Convo-
lutional Neural Networks. In IEEE Intl. Conf. on Robotics
and Automation (ICRA), 2017. 1
[32] Donald A. Norman. The Design of Everyday Things. Basic
Books, Inc., USA, 2002. 3
[33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
Krueger, and Ilya Sutskever.
Learning transferable visual
models from natural language supervision. In Intl. Conf. on
Machine Learning (ICML), pages 8748–8763. PMLR, 2021.
2
[34] Aaron Ray,
Christopher Bradley,
Luca Carlone,
and
Nicholas Roy. Task and motion planning in hierarchical 3d
scene graphs. arXiv preprint arXiv:2403.08094, 2024. 3
[35] A. Rosinol, A. Violette, M. Abate, N. Hughes, Y. Chang,
J. Shi, A. Gupta, and L. Carlone. Kimera: from SLAM to
spatial perception with 3D dynamic scene graphs. Intl. J. of
Robotics Research, 40(12–14):1510–1546, 2021. 1, 2
[36] R. F. Salas-Moreno, R. A. Newcombe, H. Strasdat, P. H. J.
Kelly, and A. J. Davison. SLAM++: Simultaneous localisa-
tion and mapping at the level of objects. In IEEE Conf. on
Computer Vision and Pattern Recognition (CVPR), 2013. 1
[37] Nur Muhammad Mahi Shafiullah, Chris Paxton, Lerrel
Pinto, Soumith Chintala, and Arthur Szlam.
Clip-fields:
Weakly supervised semantic fields for robotic memory.
arXiv preprint arXiv:2210.05663, 2022. 2
[38] M. Shan, Q. Feng, and N. Atanasov. Object residual con-
strained visual-inertial odometry.
In IEEE/RSJ Intl. Conf.
on Intelligent Robots and Systems (IROS), pages 5104–5111,
2020. 1
[39] Olaolu Shorinwa, Johnathan Tucker, Aliyah Smith, Aiden
Swann,
Timothy Chen,
Roya Firoozi,
Monroe David
Kennedy, and Mac Schwager.
Splat-mover: Multi-stage,
open-vocabulary robotic manipulation via editable gaussian
splatting.
In 8th Annual Conference on Robot Learning,
2024. 2
[40] Tom Silver, Soham Dan, Kavitha Srinivas, Joshua B. Tenen-
baum, Leslie Pack Kaelbling, and Michael Katz. General-
ized planning in pddl domains with pretrained large language
models. In Nat. Conf. on Artificial Intelligence (AAAI), 2023.
3
[41] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal,
Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason,
and Animesh Garg. Progprompt: Generating situated robot
task plans using large language models. In IEEE Intl. Conf.
on Robotics and Automation (ICRA), pages 11523–11530,
2023. 3
[42] Naftali Tishby, Fernando Pereira, and William Bialek. The
information bottleneck method. Proc. of the Allerton Con-
ference on Communication, Control and Computation, 49,
2001. 2, 4, 8
[43] Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, and
Subbarao Kambhampati. Planbench: An extensible bench-
mark for evaluating large language models on planning and
reasoning about change. In Conf. on Neural Information Pro-
cessing Systems (NeurIPS), 2022. 3
[44] Pavan Kumar Anasosalu Vasu, Hadi Pouransari, Fartash
Faghri, Raviteja Vemulapalli, and Oncel Tuzel.
Mobile-
clip: Fast image-text models through multi-modal reinforced
training. arXiv preprint: 2311.17049, abs/2311.17049, 2023.
3
[45] Abdelrhman Werby, Chenguang Huang, Martin Büchner,
Abhinav Valada, and Wolfram Burgard. Hierarchical open-
vocabulary 3d scene graphs for language-grounded robot
navigation. Robotics: Science and Systems (RSS), 2024. 1,
2, 7
[46] S. Wu, J. Wald, K. Tateno, N. Navab, and F. Tombari. Scene-
GraphFusion: Incremental 3D scene graph prediction from
RGB-D sequences. In IEEE Conf. on Computer Vision and
Pattern Recognition (CVPR), pages 7515–7525, 2021. 1, 2
[47] Karmesh Yadav, Santhosh Kumar Ramakrishnan, John
Turner, Aaron Gokaslan, Oleksandr Maksymets, Rishabh
Jain, Ram Ramrakhya, Angel X Chang, Alexander Clegg,
Manolis Savva, Eric Undersander, Devendra Singh Chap-
lot, and Dhruv Batra.
Habitat challenge 2022.
https:
//aihabitat.org/challenge/2022/, 2022. 6, 1,
2
[48] Zhutian Yang, Caelan Garrett, Dieter Fox, Tomás Lozano-
Pérez, and Leslie Pack Kaelbling. Guiding long-horizon task
and motion planning with vision language models.
arXiv
preprint arXiv:2410.02193, 2024. 3
[49] N. Yokoyama, S. Ha, D. Batra, J. Wang, and B. Bucher.
VLFM: Vision-language frontier maps for zero-shot seman-
10
