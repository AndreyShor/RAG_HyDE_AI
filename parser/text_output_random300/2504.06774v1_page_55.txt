Figure A5: Energy contained in the first ten POD modes from the turbulent experimental
cylinder dataset.
niques such as stratified sampling or data augmentation can ensure that all classes
are adequately represented in each set, preserving the integrity of the evaluation
process [57].
• Cross-Validation for Limited Data: In cases where data is scarce, cross-validation
can be employed to maximize the use of available data while ensuring robust evalu-
ation [47]. By rotating the dev and test sets across folds, the model’s performance
can be assessed comprehensively.
A.3.2
Normalization and Vectorization
Normalization is a crucial preprocessing step in neural network optimization, especially for
deep learning applications. Normalization plays a critical role in mitigating the vanishing
and exploding gradient problems in deep networks. In very deep architectures, gradients
can shrink or grow exponentially as they propagate through layers, leading to inefficient
or unstable training. Normalization helps stabilize the gradient flow by ensuring that the
input values are within a range suitable for neural network computations, thus reducing
the risk of extreme gradient behavior [47].
The choice of an appropriate normalization technique is critical for ensuring accurate
model performance. For comparison, Z-score and min-max normalization were imple-
mented and examined across each case. The results indicate that min-max normalization
performed significantly worse for both the laminar and turbulent datasets. As shown in
Figure A7, the spanwise predictions generated by the SVD-based LSTM 1 and 2 dense
models for the 3D cylinder case fail to capture the flow features and dynamics, with
RRMSE values exceeding 70%. These findings underscore the importance of selecting an
55
