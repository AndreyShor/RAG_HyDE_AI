(c) 
 
Figure 2. (a) Cross-section of MTJ [24], (b) Parallel and Anti-parallel states of MTJ [24], (c) Probabilities of 1s and 0s (parallel 
and antiparallel states) generated by an MTJ under different bias voltages [24].   
 
III. 
STRUCTURE OF DEEP NEURAL NETWORK AND QUANTIZATION OF SYNAPSES AND OUTPUTS 
Two neural networks were implemented so the performance between the quantized and high precision implementation 
could be compared for a single hidden layer network and deeper network with three hidden layers. The input layer for 
both consists of 28Ã—28 =784 pixels or data points. Architectures are summarized in Table I below. The network with 
one hidden layer is visualized in Figure 1. 
 
Table 1: Architectures of one hidden layer and three hidden layer neural networks 
Neural network-1: One hidden layer: 
 
Input layer 
nodes  
Hidden 
layer-1 
Output layer 
nodes 
784 
128 
10 
Neural network-2: Three hidden layers: 
 
Input 
layer 
Hidden 
layer-1 
Hidden 
layer-2 
Hidden 
layer-3 
Output 
layer 
784 
392 
196 
98 
10 
 
 
Figure 3: Neural network architecture with one hidden layer. The input data points are multiplied by their 
corresponding weights, followed by the application of a sigmoid activation function to generate the hidden layer nodes. 
Similarly, the output nodes are computed from the hidden layer. In our design, the network consists of 784 input 
neurons, 128 hidden neurons, and 10 output neurons (n = 784, m = 128, k = 10). 
