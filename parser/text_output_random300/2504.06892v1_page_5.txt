5
FIG. 2: QAE–qudit VQC model. (a) A schematic representation of the QAE, trained to create an adapted mapping: feature
K-plets to D-plets. For our case study, K = 5 and D = 80. This model consists of classical trainable encoding and decoding
layers, with a quantum feature map in the bottleneck. The QAE is trained to minimize the MSE between input and output.
After training, the output values χ from the D neurons (second layer) are used as input features to the VQC shown in (b).
The VQC operates on a qudit with d = 9 and is formed by a sequence of basic VQC units, ˆU(⃗ϕl, ⃗x), with l = 1, . . . 8, (8). The
D-plet of features provided by the QAE, is re-uploaded on each of these units.
The structure of the QAE that we use is presented in
the Fig. 2 (a). The classical encoding part consists by
an input layer of K = 5 neurons and a second layer of
D = 80 neurons. The neurons of the two layers are fully
connected with linear weights θ. The χi outputs of the
second layer are used to encode the quantum layer of
QAE and generate the state
|χ⟩= exp

−i
D
X
j=1
χjˆgj

|0⟩
.
(10)
We then use the d = 9 complex amplitudes of the state
|χ⟩over qudit’s computational basis to create an input to
the classical decoder consisting of a first layer of 2d = 18
neurons and an output layer of K neurons.
The two
decoding layers are also fully connected to each other in
a linear way. The QAE is trained with the train data so
that MSE (7) is minimized. After this step, we use the
QAE with the optimized weights θ to create for each K-
plet of features, that is the input to the QAE, a sequence
of D features xj which are identified the outputs χ of the
second classical layer.
The classification results for the test data, obtained
using the combined method of QAE and qudit VQC,
are reported in Table II. These results are comparable
to those achieved by the deep NN, as shown in Table I.
The number of trainable weights in the NN is approxi-
mately 6.7×104, while in the proposed QAE–qudit VQC
model model, it is approximately 103. Additionally, our
proposed method decomposes into two independent op-
timization proceduresone for the QAE and one for the
qudit VQC. For these results, we numerically simulated
the qudit VQC and successfully processed the consider-
ably large dataset for QML standards [23, 24], consisting
of around 10, 000 rows, with a runtime approximately
≈10 times slower than that of the deep NN.
B.
Secondary models of lower performance
The results in Table II are encouraging, demonstrat-
ing very good performance for the proposed model on
the given dataset. However, one might question whether
the QAE and qudit structure of the VQC, as described
