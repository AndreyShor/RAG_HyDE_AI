• Comparisons to classical geometry: Are there
direct analogies between, say, constraints in
Sym+(n) and constraints in low-rank entan-
gled states? Might we systematically port clas-
sical manifold algorithms to the entangled set-
ting?
Better theoretical understanding could guide the
design of quantum architectures that exploit ge-
ometry for tasks that classical GML cannot handle
as effectively. This line of research merges differ-
ential geometry with quantum information theory
and stands to deepen both fields.
5.6. Convergence of Classical GML and QML
Toolkits
Finally, an overarching theme is that classical
GML and QML should not be seen as competitors
but as two ends of a spectrum of geometry-aware
methods. We anticipate:
• Shared
software
libraries:
Tools
like
Geomstats (for classical Riemannian geom-
etry) and quantum frameworks (PennyLane,
Qiskit) may converge,
offering integrated
manifold layers for SPD, Grassmann,
and
quantum states.
• Theoretically unified course curricula: Future
educational material might teach GML and
QML geometry side by side, emphasizing how
quantum state spaces generalize classical man-
ifold concepts.
This convergence could accelerate progress, high-
lighting synergy rather than fragmentation in the
broader geometry-oriented ML community.
6. Conclusion
Quantum Machine Learning (QML) can be
viewed, at its core, as a specialized continuation of
the geometric machine learning (GML) tradition,
one in which the manifold of interest is the space of
quantum states. By employing metrics like the Fu-
bini -Study (for pure states) or the Bures/Helstrom
(for mixed states), QML respects the intrinsic cur-
vature arising from superposition, entanglement,
and interference, much as classical GML respects
the curvature of SPD or Grassmann manifolds.
Our discussion has illuminated how QML’s po-
tential advantages over classical methods can be un-
derstood through geometric lenses. The geometry
of quantum states is inherently high-dimensional
and shaped by uniquely quantum constraints,
which can lead to more expressive kernels, stronger
representational capacity, or more efficient training
dynamics (via quantum natural gradients).
In the near term, hybrid quantum -classical ap-
proaches appear most promising. As demonstrated
in diabetic foot ulcer classification and structural
health monitoring, classical manifold-based feature
extraction can be combined with quantum embed-
dings to deliver performance gains even on NISQ
hardware.
Looking ahead, the advent of fault-
tolerant quantum devices and advanced integrative
algorithms could unify classical and quantum geom-
etry even more deeply, opening the door to quan-
tum large language models, quantum reinforcement
learning, and other far-reaching applications.
The key takeaway is that QML is not a radical
departure from the geometric principles proven so
effective in classical ML, but rather an extension
that embraces the unique curvature and dimension-
ality of quantum state spaces. We hope the per-
spective presented here clarifies the connection be-
tween GML and QML, fosters cross-pollination of
ideas, and encourages more researchers to explore
the manifold-based foundations of quantum com-
putation, paving the way for the next generation of
machine learning breakthroughs.
Acknowledgments
During the preparation of this work, the au-
thor(s) used OpenAI in order to improve the lan-
guage. After using this service, the author(s) re-
viewed and edited the content as needed and take(s)
full responsibility for the content of the publication.
References
[1] O. Tuzel, F. Porikli, and P. Meer,
Pedestrian detec-
tion via classification on Riemannian manifolds, IEEE
Transactions on Pattern Analysis and Machine Intel-
ligence, 30(10):1713–1727, 2008.
[2] X. Pennec, P. Fillard, and N. Ayache, A Riemannian
framework for tensor computing, International Journal
of Computer Vision, 66(1):41–66, 2006.
[3] J. Hamm and D. Lee, Grassmann discriminant analysis:
A unifying view on subspace-based learning, in ICML,
2008.
[4] P. Turaga,
A. Veeraraghavan,
A. Srivastava,
and
R. Chellappa, Statistical computations on Grassmann
17
